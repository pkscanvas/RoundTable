{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective:\n",
    "Notebook to generate index for the table/dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import importlib\n",
    "from typing import Optional\n",
    "from whoosh.index import create_in\n",
    "from whoosh.fields import Schema, TEXT, ID\n",
    "from whoosh.qparser import QueryParser, FuzzyTermPlugin\n",
    "DATA_PATH = '/Users/kumarp05/table_talks/data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the file    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Group</th>\n",
       "      <th>GLOBAL CUSTOMER FULL</th>\n",
       "      <th>ActualNetSales_sum</th>\n",
       "      <th>ActualQty_sum</th>\n",
       "      <th>GroupContributionToOurRevenue%</th>\n",
       "      <th>UnitPrice</th>\n",
       "      <th>UnitPriceDiff</th>\n",
       "      <th>created_on</th>\n",
       "      <th>billed_on</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Core Products</td>\n",
       "      <td>NOVO NORDISK AS</td>\n",
       "      <td>98086461.33</td>\n",
       "      <td>5000494.410</td>\n",
       "      <td>84.476</td>\n",
       "      <td>19.615353</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thursday, 12 January 2023</td>\n",
       "      <td>2000-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HVP</td>\n",
       "      <td>NOVO NORDISK AS</td>\n",
       "      <td>18025645.78</td>\n",
       "      <td>494998.814</td>\n",
       "      <td>15.524</td>\n",
       "      <td>36.415533</td>\n",
       "      <td>16.800181</td>\n",
       "      <td>Friday, 13 January 2023</td>\n",
       "      <td>2000-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Core Products</td>\n",
       "      <td>INIBSA</td>\n",
       "      <td>12677914.26</td>\n",
       "      <td>1129776.959</td>\n",
       "      <td>96.992</td>\n",
       "      <td>11.221608</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Saturday, 14 January 2023</td>\n",
       "      <td>2000-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HVP</td>\n",
       "      <td>INIBSA</td>\n",
       "      <td>393157.99</td>\n",
       "      <td>10450.237</td>\n",
       "      <td>3.008</td>\n",
       "      <td>37.621921</td>\n",
       "      <td>26.400313</td>\n",
       "      <td>Sunday, 15 January 2023</td>\n",
       "      <td>2000-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Core Products</td>\n",
       "      <td>YICHANG RENFU MEDICINE</td>\n",
       "      <td>581579.18</td>\n",
       "      <td>29382.000</td>\n",
       "      <td>98.681</td>\n",
       "      <td>19.793723</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Monday, 16 January 2023</td>\n",
       "      <td>2000-01-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Group    GLOBAL CUSTOMER FULL  ActualNetSales_sum  ActualQty_sum  \\\n",
       "0  Core Products         NOVO NORDISK AS         98086461.33    5000494.410   \n",
       "1            HVP         NOVO NORDISK AS         18025645.78     494998.814   \n",
       "2  Core Products                  INIBSA         12677914.26    1129776.959   \n",
       "3            HVP                  INIBSA           393157.99      10450.237   \n",
       "4  Core Products  YICHANG RENFU MEDICINE           581579.18      29382.000   \n",
       "\n",
       "   GroupContributionToOurRevenue%  UnitPrice  UnitPriceDiff  \\\n",
       "0                          84.476  19.615353            NaN   \n",
       "1                          15.524  36.415533      16.800181   \n",
       "2                          96.992  11.221608            NaN   \n",
       "3                           3.008  37.621921      26.400313   \n",
       "4                          98.681  19.793723            NaN   \n",
       "\n",
       "                  created_on  billed_on  \n",
       "0  Thursday, 12 January 2023 2000-01-01  \n",
       "1    Friday, 13 January 2023 2000-01-02  \n",
       "2  Saturday, 14 January 2023 2000-01-03  \n",
       "3    Sunday, 15 January 2023 2000-01-04  \n",
       "4    Monday, 16 January 2023 2000-01-05  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_file(file_name: str) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Function to read different formats of spreadsheets like CSV, XLS, XLSX, and Parquet based on the extension of the file uploaded\n",
    "\n",
    "    Args:\n",
    "        file_name (str): The name of the spreadsheet file to be read\n",
    "\n",
    "    Returns:\n",
    "        Optional[pd.DataFrame]: A DataFrame object containing the spreadsheet data, or None if the file format is not supported\n",
    "\n",
    "    Raises:\n",
    "        NotImplementedError: If an unsupported file format is passed as a parameter\"\"\"\n",
    "\n",
    "    # Determine the file format from its extension\n",
    "    file_ext = file_name.split(\".\")[-1].lower()\n",
    "\n",
    "    if file_ext == \"csv\":\n",
    "        df = pd.read_csv(file_name)\n",
    "    elif file_ext == \"xls\" or file_ext == \"xlsx\":\n",
    "        df = pd.read_excel(file_name)\n",
    "    elif file_ext == \"parquet\":\n",
    "        df = pd.read_parquet(file_name)\n",
    "    else:\n",
    "        print(f\"Unsupported file format: {file_ext}\")\n",
    "        return None\n",
    "\n",
    "    # Return the DataFrame containing the spreadsheet data\n",
    "    return df\n",
    "    \n",
    "file_name = 'core_hvp_diff.xlsx'\n",
    "df = read_file(file_name)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Dtypes of the table:\n",
    "- TODO: Automatic inferring\n",
    "- TODO: Manual changing option of dtypes of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Group': dtype('O'), 'GLOBAL CUSTOMER FULL': dtype('O'), 'ActualNetSales_sum': dtype('float64'), 'ActualQty_sum': dtype('float64'), 'GroupContributionToOurRevenue%': dtype('float64'), 'UnitPrice': dtype('float64'), 'UnitPriceDiff': dtype('float64'), 'created_on': dtype('O'), 'billed_on': dtype('<M8[ns]')}\n"
     ]
    }
   ],
   "source": [
    "def get_dtypes(df: pd.DataFrame) -> dict[str, type]:\n",
    "    \"\"\"Function to get the data types of each column in a DataFrame and store them in a dictionary with column name as key and dtype as value\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The Pandas DataFrame object for which we want to get the data types\n",
    "\n",
    "    Returns:\n",
    "        dict[str, type]: A dictionary containing the column names and corresponding data types of the input DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the dtypes of each column in the input DataFrame\n",
    "    column_dtypes = df.dtypes.to_dict()\n",
    "\n",
    "    # Rename the keys to be the column names instead of the dtype names\n",
    "    col_name_dtypes = {k: v for k, v in column_dtypes.items()}\n",
    "\n",
    "    # Return the dictionary containing the data types of each column\n",
    "    return col_name_dtypes\n",
    "\n",
    "data_type = get_dtypes(df)\n",
    "print(data_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Unique Values\n",
    "- Of Categorical Columns\n",
    "- TODO: Option to avoid indexing ID columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Group': ['Core Products', 'HVP'],\n",
       " 'GLOBAL CUSTOMER FULL': ['NOVO NORDISK AS',\n",
       "  'INIBSA',\n",
       "  'YICHANG RENFU MEDICINE',\n",
       "  'BAXTER INTERNATIONAL',\n",
       "  'BECTON DICKINSON & COMPANY',\n",
       "  'HUONS - KOREA',\n",
       "  'B.BRAUN MELSUNGEN AG',\n",
       "  'REIG JOFRE GROUP',\n",
       "  'JSC FARMAK',\n",
       "  'SQUARE PHARMACEUTICALS LIMITED',\n",
       "  'LABORATORIO P. CASSARA S.R.L.',\n",
       "  'LYOCONTRACT - GLOBAL',\n",
       "  'ISO-TEX DIAGNOSTICS INC',\n",
       "  'VALDEPHARM',\n",
       "  'DWK LIFE SCIENCES LIMITED',\n",
       "  'TRICORBRAUN',\n",
       "  'APSEN FARMACEUTICA S/A',\n",
       "  'SMALL VOLUME - CORE',\n",
       "  'LG CHEMICAL - KOREA',\n",
       "  'EUROFARMA',\n",
       "  'GEN ILAC VE SAGLIK √úR√úNLERI AS.',\n",
       "  'SINTETICA SA',\n",
       "  'CURIUM US LLC',\n",
       "  'NEON LABORATORIES LTD',\n",
       "  'ZOETIS',\n",
       "  'SIEMENS AG',\n",
       "  'PT SINAR GOLDSINDO',\n",
       "  'EVAPHARMA FOR PHARMACEUTICAL',\n",
       "  'G.S. COSMECEUTICAL USA INC',\n",
       "  'CARDINAL HEALTH',\n",
       "  'INDUSTRIAL VETERINARIA, S.A.',\n",
       "  'GE HEALTHCARE',\n",
       "  'WPS AFFILIATES',\n",
       "  'ALCON RESEARCH LLC',\n",
       "  'GRIFOLS INC',\n",
       "  'STELIS BIOPHARMA LIMITED',\n",
       "  'NCPC BIOTECH SUPPLIES BRANCH',\n",
       "  'EMAD TRADE HOUSE FZCO',\n",
       "  'RECIPHARM',\n",
       "  'FERRING PHARMACEUTICALS',\n",
       "  'INCEPTA PHARMA - GLOBAL',\n",
       "  'STALLERGENES SAS',\n",
       "  'DONGKOOK PHARMACEUTICAL',\n",
       "  'BIOTEST',\n",
       "  'DONG-A ST CHEONAN PLANT',\n",
       "  'MALLINCKRODT',\n",
       "  'BELL-MORE LABS INC',\n",
       "  'CEVA ANIMAL HEALTH PTY LTD',\n",
       "  'SOLID PHASE INC',\n",
       "  'LABORATORIOS SYVA, S.A',\n",
       "  'FAMAR HEALTH CARE SERVICES MADRID S.A.U.',\n",
       "  'BIO-RAD',\n",
       "  'CHIA TAI TIANQING PHARMACEUTICAL GR',\n",
       "  'BIOMED CO LTD',\n",
       "  'REVALESIO CORPORATION',\n",
       "  'HANHUI PHARMACEUTICALS CO., LTD.',\n",
       "  'JULIO FORNARO S.A.',\n",
       "  'MS PHARMA/JORDAN',\n",
       "  'ALTAN PHARMACEUTICALS S.A.U.',\n",
       "  'GUIZHOU JINGFENG INJECTION',\n",
       "  'CRISTALIA',\n",
       "  'CSPC BAIKE',\n",
       "  'OOO PHARMTECHSERVICE',\n",
       "  'RELIANCE LIFE SCIENCES PVT. LTD',\n",
       "  'CSPC REFLOW PHARMACEUTICAL',\n",
       "  'HILTON PHARMA',\n",
       "  'POLIFARMA ILAC SAN. VE TIC. A.S',\n",
       "  'KHANDELWAL LAB. PVT. LTD',\n",
       "  'AMPHASTAR PHARMACEUTICALS',\n",
       "  'GLOBAL MEDICAL SOLUTIONS PL',\n",
       "  'BEXIMCO PHARMACEUTICALS LIMITED',\n",
       "  'WOCKHARDT LTD',\n",
       "  'SHANGHAI JINGFENG PHARMA',\n",
       "  'AJPHARMCOPUNCTURE',\n",
       "  'LABORATORIO LKM S.A.',\n",
       "  'PALLEON PHARMACEUTICALS, INC',\n",
       "  'DELPHARM SAINT REMY SAS',\n",
       "  'ISDO, S.L.',\n",
       "  'GREER LABORATORIES INC',\n",
       "  'KLOSTERFRAU HEALTHCARE GROUP',\n",
       "  'MAVLAB ANIMAL HEALTH PTY LTD',\n",
       "  'GETWELL PHARMACEUTICALS',\n",
       "  'HEMOFARM A.D.',\n",
       "  'M/S. JULPHAR, GULF PHARMACEUTICAL',\n",
       "  'EUBIOLOGICS CO., LTD.',\n",
       "  'G.L .PHARMA GMBH',\n",
       "  'CADILA PHARMACEUTICALS LTD',\n",
       "  'HIPRA',\n",
       "  'ONKO',\n",
       "  'BHARAT SERUMS & VACCINES',\n",
       "  'MP BIOMEDICALS LLC',\n",
       "  'FACET ANALYTICAL SERVICES',\n",
       "  'TECNOVAX S.A.',\n",
       "  'CREOSALUS INC',\n",
       "  'LABORATORIOS NORMON SA',\n",
       "  'HEALTHCARE PHARMACEUTICALS LTD.',\n",
       "  'GERRESHEIMER BUNDE',\n",
       "  'DIASORIN INC',\n",
       "  'HANLIM PHARM',\n",
       "  'BIOMERIEUX - GLOBAL',\n",
       "  'SEPTODONT/NOVOCOL',\n",
       "  'ZHEJIANG HUAHAI BIOTECHNOLOGY',\n",
       "  'VITAMED D.O.O.',\n",
       "  'REVOLUTION COSMETICS USA PTE LTD',\n",
       "  'ZSCHEILE & KLINGER GMBH',\n",
       "  'BERLIN PACKAGING',\n",
       "  'SOTHEMA',\n",
       "  'BMIKOREA',\n",
       "  'PROMEGA',\n",
       "  'SHENZHEN TECHDOW PHARMACEUTICA',\n",
       "  'SOLUPHARM PHARMA - GLOBAL',\n",
       "  'AENOVA GROUP',\n",
       "  'ANHUI ANKE BIOTECHNOLOGY',\n",
       "  'ADVANCED CHEMICAL INDUSTRIES LTD',\n",
       "  'PHARMEDIPACKDIREKT',\n",
       "  'AKESO BIOPHARMA, INC',\n",
       "  'HANGZHOU ZHONGMEI HUADONG',\n",
       "  'ARISTOPHARMA LIMITED',\n",
       "  'APODANNORDIC PHARMAPACKAGING A',\n",
       "  'SALUTE VITA FARMACIA DE MANIPULACAO LTDA',\n",
       "  'KOREA UNITED PHARM. INC.',\n",
       "  'NORDIC PACK AB',\n",
       "  'OSEL ILAC SANAYI VE TICARET AS',\n",
       "  'ELANCO',\n",
       "  'ALK-ABELLO, S.A.',\n",
       "  'HARALD MARKMANN',\n",
       "  'ZHAOKE PHARMACEUTICAL',\n",
       "  'SHENZHEN JIANXIANG BIOPHARMACEUTICAL',\n",
       "  \"ST. JUDE CHILDREN'S RESEARCH\",\n",
       "  'CHENGDU GINEBIO BIOLOGICAL TECHNOLOGY',\n",
       "  'FIDIA FARMACEUTICI SPA',\n",
       "  'LEIDOS BIOMEDICAL RESEARCH INC',\n",
       "  'LAB. DE PRODUCTOS ETICOS CEISA',\n",
       "  'CONTACARE OPTHALMICS & DIAGNOSTICS',\n",
       "  'SHANGHAI YUAN LAI TRADING CO.,LTD',\n",
       "  'CKD BIO CORP.',\n",
       "  'VHG LABS INC',\n",
       "  'VIRCHOW BIOTECH PVT LTD',\n",
       "  'WEGO GROUP',\n",
       "  'ABBOTT',\n",
       "  'TRIRX SEGRE SAS',\n",
       "  'FUJIREBIO DIAGNOSTICS INC',\n",
       "  'LES LABORATOIRES MEDIS S.A.',\n",
       "  'MARCHESINI GROUP SPA',\n",
       "  'GROUPE PARIMA INC',\n",
       "  'PANACEA BIOTEC LTD',\n",
       "  'INJEMED MEDICAMENTOS ESPECIAIS LTDA',\n",
       "  'ATABAY',\n",
       "  'PACKAGING COORDINATORS INC',\n",
       "  'PROD.FARMACEUT.DR. GRAY SACI',\n",
       "  'RESILIENCE BIOTECHNOLOGIES INC',\n",
       "  'MASSACHUSETTS BIOLOGICS',\n",
       "  'TCOAG IRELAND LTD',\n",
       "  'WINTAC',\n",
       "  'QUIMFA S.A.',\n",
       "  'WORLD  MEDICINE  ILAC SAN VE TIC AS',\n",
       "  'SHANDONG PHARMA GLASS',\n",
       "  'MERIDIAN MEDICAL TECHNOLOGIES',\n",
       "  'HELENA LABORATORIES',\n",
       "  'DAIHAN PHARM. CO., LTD.',\n",
       "  'NORBROOK LABORATORIES LTD.',\n",
       "  'ZHONGSHAN SINOBIOWAY HYGENE',\n",
       "  'TECNONUCLEAR S.A.',\n",
       "  'CALIBER THERAPEUTICS',\n",
       "  'CRONUS PHARMA SPECIALITIES INDIA',\n",
       "  'BAUSCH & LOMB INCORPORATED',\n",
       "  'UNITED CHEMICAL TECHNOLOGIES',\n",
       "  'ACS DOBFAR S.P.A.',\n",
       "  'HUA RUN ANGDE BIOCHEMICAL',\n",
       "  'LABYES S.A.',\n",
       "  'GADOR S.A.',\n",
       "  'CEDARLANE',\n",
       "  'BIOGENESIS BAGO SA',\n",
       "  'QIAGEN SCIENCES LLC',\n",
       "  'MEDIPOST CO., LTD.',\n",
       "  'SCIVAC',\n",
       "  'LEVO THERAPEUTICS, INC',\n",
       "  'IVAX ARGENTINA S.A.',\n",
       "  'ALTASCIENCES CDMO PHILADELPHIA, LLC',\n",
       "  'MEFAR ILAC SAN.  A.S.',\n",
       "  'NEPHRON SC INC',\n",
       "  'ALIDAC PHARMACEUTICALS LIMITED',\n",
       "  'QUIDEL CORP - GLOBAL',\n",
       "  'LABORATORIOS CALIER, S.A.',\n",
       "  'ALFASIGMA S.P.A.',\n",
       "  'MEDICAL UNION',\n",
       "  'BIOLYPH LLC',\n",
       "  'SHANDONG NEW TIMES PHARMA',\n",
       "  'SUZHOU ZELGEN BIOPHARMACEUTICALS',\n",
       "  'SAMARTH LIFE SCIENCES PVT LTD',\n",
       "  'TARCHOMINSKIE ZAKLADY',\n",
       "  'INNOXEL LIFESCIENCES PRIVATE LIMITED',\n",
       "  'AUG PHARMA',\n",
       "  'LEEDSAY IND. E COM. DE PROD. MED. LTDA',\n",
       "  'LABORATORIOS AC FARMA S.A.',\n",
       "  'DENOVA PHARMACEUTICAL LTDA.',\n",
       "  'INTERNATIONAL PHARMACEUTICAL IMMUNOLOGY',\n",
       "  'DCE DIETETIQUE',\n",
       "  'IBSA INSTITUT BIOCHIMIQUE SA',\n",
       "  'INST. BIOLOGICO CONTEMPORANEO',\n",
       "  'MENARINI SAGLIK VE ILAC SAN. TIC. A.S.',\n",
       "  'SLAYBACK PHARMA INDIA LLP',\n",
       "  'ICU MEDICAL INC',\n",
       "  'SOPHAL SPA ALG√âRIE',\n",
       "  'SHANGHAI FUDAN-ZHANGJIANG',\n",
       "  'WAISMAN BIOMANUFACTURING',\n",
       "  'LUCKFURT INDUSTRIE HANDELS GMBH',\n",
       "  'SHANXI ZHENDONG GROUP CO., LTD.',\n",
       "  'DEKA RESEARCH & DEVELOPMENT CORP',\n",
       "  'EVIVE BIOTECHNOLGY.,LTD',\n",
       "  'INC PHARMA',\n",
       "  'YUKON MEDICAL',\n",
       "  'SINOMAB BIOSCIENCE ÔºàHAINANÔºâ',\n",
       "  'HOLLIDAY SCOTT S.A.',\n",
       "  'JSC PHARMSTANDARD-UFAVITA',\n",
       "  'BIOSIDUS S.A.',\n",
       "  'TECNOQUIMICAS S.A.',\n",
       "  'MEDI-RADIOPHARMA LTD',\n",
       "  'SHANGHAI SUNWAY BIOTECH',\n",
       "  'KOCAK FARMA - GLOBAL',\n",
       "  'HANA PHARM CO., LTD.',\n",
       "  'PACKAGING COORDINATORS, INC',\n",
       "  'COOPERMEDICAL SRL',\n",
       "  'SMITHFIELD BIOSCIENCE INC',\n",
       "  'MICRO MEASUREMENT LABORATORIES',\n",
       "  'PHARCO COOPERATION',\n",
       "  'MEDIFARMA S.A',\n",
       "  'AMERICAN INJECTABLES INC',\n",
       "  'SARL HUPP PHARMA',\n",
       "  'SICHUAN HUI YU PHARMA LTD.,CO',\n",
       "  'ZHEJIANG POLY PHARM. CO.,LTD.',\n",
       "  'VEM ILAC SAN. VE TIC. A.S.',\n",
       "  'NYMOX PHARMACEUTICAL CORPORATION',\n",
       "  'BEIJING SHUANG LU PHARMA LTD, CO.',\n",
       "  'AL ANDALOUS FOR PHARMACEUTICAL IND.',\n",
       "  'ASTORIA-PACIFIC INC',\n",
       "  'OYSTER POINT PHARMA',\n",
       "  'LG CHEM, LTD.',\n",
       "  'MS PHARMA ƒ∞LA√á SAN. VE Tƒ∞C. A.≈û  *OBS*',\n",
       "  'SHARP PACKAGING SOLUTIONS',\n",
       "  'BLUE WATER VACCINES',\n",
       "  'J.H. RITMEESTER B.V.',\n",
       "  'BEL-ART PRODUCTS',\n",
       "  'MICROSULES ARGENTINA S.A.',\n",
       "  'MP BIOMEDICALS NEW ZEALAND LTD',\n",
       "  'SL VAXIGEN INC.',\n",
       "  'CHROMATOGRAPHIC SPECIALTIES INC',\n",
       "  'TONGYI PHARMACEUTICAL',\n",
       "  'LABORATORIOS CASASCO S.A.I.C.',\n",
       "  'SHAMROCK GLASS COMPANY',\n",
       "  'LANZHOU BIOTECHNOLOGY',\n",
       "  'SHANXI ZHENDONG PHARMACEUTICAL',\n",
       "  'SHANGHAI UNITED CELL BIOTECHNOLOGY',\n",
       "  'LAFEDAR S.A.',\n",
       "  'MESOSYSTEM SA',\n",
       "  'AMRIYA PHARMACEUTICAL',\n",
       "  'CHENGDU BRILLIANT PHARMACEUTICAL',\n",
       "  'LABORATORIO KEMEX S.A.',\n",
       "  'LEGACY PHARMACEUTICALS',\n",
       "  'IMCD CANADA LIMITED',\n",
       "  'CHENGDU YATU BIOLOGICAL TECHNOLOGY',\n",
       "  'LES LABORATOIRES BIOTECH',\n",
       "  'ESPRI ERIKA STOLLENWERK PRIM√ÑRVERPACKUNG',\n",
       "  'MTF',\n",
       "  'AALTO SCIENTIFIC',\n",
       "  'VMIC',\n",
       "  'BIOTHEUS BIOPHARMACEUTICAL',\n",
       "  'EUROMEDICINA D.O.O',\n",
       "  'SHANGHAI MIRACOGEN INC.',\n",
       "  'CIMA TECHNOLOGY INC',\n",
       "  'ONEGENE BIOTECHNOLOGY',\n",
       "  'STREULI PHARMA AG',\n",
       "  'NAVIDEA BIOPHARMACEUTICALS, INC',\n",
       "  'NEVAKAR LLC',\n",
       "  'RHOSHAN PHARMACEUTICALS',\n",
       "  'INVITRO INTERNATIONAL',\n",
       "  'MUDANJIANG YOU BO PHARMACEUTICAL',\n",
       "  'REPRODUX LABORATORIOS LTDA',\n",
       "  'SN CORP.',\n",
       "  'WATERS TECHNOLOGIES CORPORATION',\n",
       "  'CENTURION ILAC SAN.VE TIC.A.S.',\n",
       "  'MEDALLIANCE LLC',\n",
       "  'BIOMM S/A',\n",
       "  'WRIGHT MEDICAL TECHNOLOGY, INC',\n",
       "  'IMA',\n",
       "  'AQUA SCIENCE',\n",
       "  'IMMUNOGEN INC',\n",
       "  'MODERN WATER INC',\n",
       "  'PERKIN ELMER',\n",
       "  'QUALITY BIORESOURCES, INC']}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_unique_values(df: pd.DataFrame, cat_cols: list) -> dict[str, list]:\n",
    "    \"\"\"Function to return all the unique values of a given set of columns in a DataFrame\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The Pandas DataFrame object for which we want to get the unique values\n",
    "        cat_cols (str): A string containing a comma-separated list of column names for which we want to get the unique values\n",
    "\n",
    "    Returns:\n",
    "        dict[str, list]: A dictionary containing the unique values of each specified column as a list\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If any of the specified columns do not exist in the DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    # Create an empty dictionary to store the unique values of each column\n",
    "    unique_values = {}\n",
    "\n",
    "    # Iterate over each specified column and get its unique values\n",
    "    for col in cat_cols:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Column '{col}' does not exist in DataFrame\")\n",
    "        else:\n",
    "            unique_values[col] = df[col].unique().tolist()\n",
    "\n",
    "    # Return the dictionary containing the unique values of each column\n",
    "    return unique_values\n",
    "\n",
    "get_unique_values(df, ['Group','GLOBAL CUSTOMER FULL'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Categorical Columns\n",
    "- From data_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Group': ['Core Products', 'HVP'],\n",
       " 'GLOBAL CUSTOMER FULL': ['NOVO NORDISK AS',\n",
       "  'INIBSA',\n",
       "  'YICHANG RENFU MEDICINE',\n",
       "  'BAXTER INTERNATIONAL',\n",
       "  'BECTON DICKINSON & COMPANY',\n",
       "  'HUONS - KOREA',\n",
       "  'B.BRAUN MELSUNGEN AG',\n",
       "  'REIG JOFRE GROUP',\n",
       "  'JSC FARMAK',\n",
       "  'SQUARE PHARMACEUTICALS LIMITED',\n",
       "  'LABORATORIO P. CASSARA S.R.L.',\n",
       "  'LYOCONTRACT - GLOBAL',\n",
       "  'ISO-TEX DIAGNOSTICS INC',\n",
       "  'VALDEPHARM',\n",
       "  'DWK LIFE SCIENCES LIMITED',\n",
       "  'TRICORBRAUN',\n",
       "  'APSEN FARMACEUTICA S/A',\n",
       "  'SMALL VOLUME - CORE',\n",
       "  'LG CHEMICAL - KOREA',\n",
       "  'EUROFARMA',\n",
       "  'GEN ILAC VE SAGLIK √úR√úNLERI AS.',\n",
       "  'SINTETICA SA',\n",
       "  'CURIUM US LLC',\n",
       "  'NEON LABORATORIES LTD',\n",
       "  'ZOETIS',\n",
       "  'SIEMENS AG',\n",
       "  'PT SINAR GOLDSINDO',\n",
       "  'EVAPHARMA FOR PHARMACEUTICAL',\n",
       "  'G.S. COSMECEUTICAL USA INC',\n",
       "  'CARDINAL HEALTH',\n",
       "  'INDUSTRIAL VETERINARIA, S.A.',\n",
       "  'GE HEALTHCARE',\n",
       "  'WPS AFFILIATES',\n",
       "  'ALCON RESEARCH LLC',\n",
       "  'GRIFOLS INC',\n",
       "  'STELIS BIOPHARMA LIMITED',\n",
       "  'NCPC BIOTECH SUPPLIES BRANCH',\n",
       "  'EMAD TRADE HOUSE FZCO',\n",
       "  'RECIPHARM',\n",
       "  'FERRING PHARMACEUTICALS',\n",
       "  'INCEPTA PHARMA - GLOBAL',\n",
       "  'STALLERGENES SAS',\n",
       "  'DONGKOOK PHARMACEUTICAL',\n",
       "  'BIOTEST',\n",
       "  'DONG-A ST CHEONAN PLANT',\n",
       "  'MALLINCKRODT',\n",
       "  'BELL-MORE LABS INC',\n",
       "  'CEVA ANIMAL HEALTH PTY LTD',\n",
       "  'SOLID PHASE INC',\n",
       "  'LABORATORIOS SYVA, S.A',\n",
       "  'FAMAR HEALTH CARE SERVICES MADRID S.A.U.',\n",
       "  'BIO-RAD',\n",
       "  'CHIA TAI TIANQING PHARMACEUTICAL GR',\n",
       "  'BIOMED CO LTD',\n",
       "  'REVALESIO CORPORATION',\n",
       "  'HANHUI PHARMACEUTICALS CO., LTD.',\n",
       "  'JULIO FORNARO S.A.',\n",
       "  'MS PHARMA/JORDAN',\n",
       "  'ALTAN PHARMACEUTICALS S.A.U.',\n",
       "  'GUIZHOU JINGFENG INJECTION',\n",
       "  'CRISTALIA',\n",
       "  'CSPC BAIKE',\n",
       "  'OOO PHARMTECHSERVICE',\n",
       "  'RELIANCE LIFE SCIENCES PVT. LTD',\n",
       "  'CSPC REFLOW PHARMACEUTICAL',\n",
       "  'HILTON PHARMA',\n",
       "  'POLIFARMA ILAC SAN. VE TIC. A.S',\n",
       "  'KHANDELWAL LAB. PVT. LTD',\n",
       "  'AMPHASTAR PHARMACEUTICALS',\n",
       "  'GLOBAL MEDICAL SOLUTIONS PL',\n",
       "  'BEXIMCO PHARMACEUTICALS LIMITED',\n",
       "  'WOCKHARDT LTD',\n",
       "  'SHANGHAI JINGFENG PHARMA',\n",
       "  'AJPHARMCOPUNCTURE',\n",
       "  'LABORATORIO LKM S.A.',\n",
       "  'PALLEON PHARMACEUTICALS, INC',\n",
       "  'DELPHARM SAINT REMY SAS',\n",
       "  'ISDO, S.L.',\n",
       "  'GREER LABORATORIES INC',\n",
       "  'KLOSTERFRAU HEALTHCARE GROUP',\n",
       "  'MAVLAB ANIMAL HEALTH PTY LTD',\n",
       "  'GETWELL PHARMACEUTICALS',\n",
       "  'HEMOFARM A.D.',\n",
       "  'M/S. JULPHAR, GULF PHARMACEUTICAL',\n",
       "  'EUBIOLOGICS CO., LTD.',\n",
       "  'G.L .PHARMA GMBH',\n",
       "  'CADILA PHARMACEUTICALS LTD',\n",
       "  'HIPRA',\n",
       "  'ONKO',\n",
       "  'BHARAT SERUMS & VACCINES',\n",
       "  'MP BIOMEDICALS LLC',\n",
       "  'FACET ANALYTICAL SERVICES',\n",
       "  'TECNOVAX S.A.',\n",
       "  'CREOSALUS INC',\n",
       "  'LABORATORIOS NORMON SA',\n",
       "  'HEALTHCARE PHARMACEUTICALS LTD.',\n",
       "  'GERRESHEIMER BUNDE',\n",
       "  'DIASORIN INC',\n",
       "  'HANLIM PHARM',\n",
       "  'BIOMERIEUX - GLOBAL',\n",
       "  'SEPTODONT/NOVOCOL',\n",
       "  'ZHEJIANG HUAHAI BIOTECHNOLOGY',\n",
       "  'VITAMED D.O.O.',\n",
       "  'REVOLUTION COSMETICS USA PTE LTD',\n",
       "  'ZSCHEILE & KLINGER GMBH',\n",
       "  'BERLIN PACKAGING',\n",
       "  'SOTHEMA',\n",
       "  'BMIKOREA',\n",
       "  'PROMEGA',\n",
       "  'SHENZHEN TECHDOW PHARMACEUTICA',\n",
       "  'SOLUPHARM PHARMA - GLOBAL',\n",
       "  'AENOVA GROUP',\n",
       "  'ANHUI ANKE BIOTECHNOLOGY',\n",
       "  'ADVANCED CHEMICAL INDUSTRIES LTD',\n",
       "  'PHARMEDIPACKDIREKT',\n",
       "  'AKESO BIOPHARMA, INC',\n",
       "  'HANGZHOU ZHONGMEI HUADONG',\n",
       "  'ARISTOPHARMA LIMITED',\n",
       "  'APODANNORDIC PHARMAPACKAGING A',\n",
       "  'SALUTE VITA FARMACIA DE MANIPULACAO LTDA',\n",
       "  'KOREA UNITED PHARM. INC.',\n",
       "  'NORDIC PACK AB',\n",
       "  'OSEL ILAC SANAYI VE TICARET AS',\n",
       "  'ELANCO',\n",
       "  'ALK-ABELLO, S.A.',\n",
       "  'HARALD MARKMANN',\n",
       "  'ZHAOKE PHARMACEUTICAL',\n",
       "  'SHENZHEN JIANXIANG BIOPHARMACEUTICAL',\n",
       "  \"ST. JUDE CHILDREN'S RESEARCH\",\n",
       "  'CHENGDU GINEBIO BIOLOGICAL TECHNOLOGY',\n",
       "  'FIDIA FARMACEUTICI SPA',\n",
       "  'LEIDOS BIOMEDICAL RESEARCH INC',\n",
       "  'LAB. DE PRODUCTOS ETICOS CEISA',\n",
       "  'CONTACARE OPTHALMICS & DIAGNOSTICS',\n",
       "  'SHANGHAI YUAN LAI TRADING CO.,LTD',\n",
       "  'CKD BIO CORP.',\n",
       "  'VHG LABS INC',\n",
       "  'VIRCHOW BIOTECH PVT LTD',\n",
       "  'WEGO GROUP',\n",
       "  'ABBOTT',\n",
       "  'TRIRX SEGRE SAS',\n",
       "  'FUJIREBIO DIAGNOSTICS INC',\n",
       "  'LES LABORATOIRES MEDIS S.A.',\n",
       "  'MARCHESINI GROUP SPA',\n",
       "  'GROUPE PARIMA INC',\n",
       "  'PANACEA BIOTEC LTD',\n",
       "  'INJEMED MEDICAMENTOS ESPECIAIS LTDA',\n",
       "  'ATABAY',\n",
       "  'PACKAGING COORDINATORS INC',\n",
       "  'PROD.FARMACEUT.DR. GRAY SACI',\n",
       "  'RESILIENCE BIOTECHNOLOGIES INC',\n",
       "  'MASSACHUSETTS BIOLOGICS',\n",
       "  'TCOAG IRELAND LTD',\n",
       "  'WINTAC',\n",
       "  'QUIMFA S.A.',\n",
       "  'WORLD  MEDICINE  ILAC SAN VE TIC AS',\n",
       "  'SHANDONG PHARMA GLASS',\n",
       "  'MERIDIAN MEDICAL TECHNOLOGIES',\n",
       "  'HELENA LABORATORIES',\n",
       "  'DAIHAN PHARM. CO., LTD.',\n",
       "  'NORBROOK LABORATORIES LTD.',\n",
       "  'ZHONGSHAN SINOBIOWAY HYGENE',\n",
       "  'TECNONUCLEAR S.A.',\n",
       "  'CALIBER THERAPEUTICS',\n",
       "  'CRONUS PHARMA SPECIALITIES INDIA',\n",
       "  'BAUSCH & LOMB INCORPORATED',\n",
       "  'UNITED CHEMICAL TECHNOLOGIES',\n",
       "  'ACS DOBFAR S.P.A.',\n",
       "  'HUA RUN ANGDE BIOCHEMICAL',\n",
       "  'LABYES S.A.',\n",
       "  'GADOR S.A.',\n",
       "  'CEDARLANE',\n",
       "  'BIOGENESIS BAGO SA',\n",
       "  'QIAGEN SCIENCES LLC',\n",
       "  'MEDIPOST CO., LTD.',\n",
       "  'SCIVAC',\n",
       "  'LEVO THERAPEUTICS, INC',\n",
       "  'IVAX ARGENTINA S.A.',\n",
       "  'ALTASCIENCES CDMO PHILADELPHIA, LLC',\n",
       "  'MEFAR ILAC SAN.  A.S.',\n",
       "  'NEPHRON SC INC',\n",
       "  'ALIDAC PHARMACEUTICALS LIMITED',\n",
       "  'QUIDEL CORP - GLOBAL',\n",
       "  'LABORATORIOS CALIER, S.A.',\n",
       "  'ALFASIGMA S.P.A.',\n",
       "  'MEDICAL UNION',\n",
       "  'BIOLYPH LLC',\n",
       "  'SHANDONG NEW TIMES PHARMA',\n",
       "  'SUZHOU ZELGEN BIOPHARMACEUTICALS',\n",
       "  'SAMARTH LIFE SCIENCES PVT LTD',\n",
       "  'TARCHOMINSKIE ZAKLADY',\n",
       "  'INNOXEL LIFESCIENCES PRIVATE LIMITED',\n",
       "  'AUG PHARMA',\n",
       "  'LEEDSAY IND. E COM. DE PROD. MED. LTDA',\n",
       "  'LABORATORIOS AC FARMA S.A.',\n",
       "  'DENOVA PHARMACEUTICAL LTDA.',\n",
       "  'INTERNATIONAL PHARMACEUTICAL IMMUNOLOGY',\n",
       "  'DCE DIETETIQUE',\n",
       "  'IBSA INSTITUT BIOCHIMIQUE SA',\n",
       "  'INST. BIOLOGICO CONTEMPORANEO',\n",
       "  'MENARINI SAGLIK VE ILAC SAN. TIC. A.S.',\n",
       "  'SLAYBACK PHARMA INDIA LLP',\n",
       "  'ICU MEDICAL INC',\n",
       "  'SOPHAL SPA ALG√âRIE',\n",
       "  'SHANGHAI FUDAN-ZHANGJIANG',\n",
       "  'WAISMAN BIOMANUFACTURING',\n",
       "  'LUCKFURT INDUSTRIE HANDELS GMBH',\n",
       "  'SHANXI ZHENDONG GROUP CO., LTD.',\n",
       "  'DEKA RESEARCH & DEVELOPMENT CORP',\n",
       "  'EVIVE BIOTECHNOLGY.,LTD',\n",
       "  'INC PHARMA',\n",
       "  'YUKON MEDICAL',\n",
       "  'SINOMAB BIOSCIENCE ÔºàHAINANÔºâ',\n",
       "  'HOLLIDAY SCOTT S.A.',\n",
       "  'JSC PHARMSTANDARD-UFAVITA',\n",
       "  'BIOSIDUS S.A.',\n",
       "  'TECNOQUIMICAS S.A.',\n",
       "  'MEDI-RADIOPHARMA LTD',\n",
       "  'SHANGHAI SUNWAY BIOTECH',\n",
       "  'KOCAK FARMA - GLOBAL',\n",
       "  'HANA PHARM CO., LTD.',\n",
       "  'PACKAGING COORDINATORS, INC',\n",
       "  'COOPERMEDICAL SRL',\n",
       "  'SMITHFIELD BIOSCIENCE INC',\n",
       "  'MICRO MEASUREMENT LABORATORIES',\n",
       "  'PHARCO COOPERATION',\n",
       "  'MEDIFARMA S.A',\n",
       "  'AMERICAN INJECTABLES INC',\n",
       "  'SARL HUPP PHARMA',\n",
       "  'SICHUAN HUI YU PHARMA LTD.,CO',\n",
       "  'ZHEJIANG POLY PHARM. CO.,LTD.',\n",
       "  'VEM ILAC SAN. VE TIC. A.S.',\n",
       "  'NYMOX PHARMACEUTICAL CORPORATION',\n",
       "  'BEIJING SHUANG LU PHARMA LTD, CO.',\n",
       "  'AL ANDALOUS FOR PHARMACEUTICAL IND.',\n",
       "  'ASTORIA-PACIFIC INC',\n",
       "  'OYSTER POINT PHARMA',\n",
       "  'LG CHEM, LTD.',\n",
       "  'MS PHARMA ƒ∞LA√á SAN. VE Tƒ∞C. A.≈û  *OBS*',\n",
       "  'SHARP PACKAGING SOLUTIONS',\n",
       "  'BLUE WATER VACCINES',\n",
       "  'J.H. RITMEESTER B.V.',\n",
       "  'BEL-ART PRODUCTS',\n",
       "  'MICROSULES ARGENTINA S.A.',\n",
       "  'MP BIOMEDICALS NEW ZEALAND LTD',\n",
       "  'SL VAXIGEN INC.',\n",
       "  'CHROMATOGRAPHIC SPECIALTIES INC',\n",
       "  'TONGYI PHARMACEUTICAL',\n",
       "  'LABORATORIOS CASASCO S.A.I.C.',\n",
       "  'SHAMROCK GLASS COMPANY',\n",
       "  'LANZHOU BIOTECHNOLOGY',\n",
       "  'SHANXI ZHENDONG PHARMACEUTICAL',\n",
       "  'SHANGHAI UNITED CELL BIOTECHNOLOGY',\n",
       "  'LAFEDAR S.A.',\n",
       "  'MESOSYSTEM SA',\n",
       "  'AMRIYA PHARMACEUTICAL',\n",
       "  'CHENGDU BRILLIANT PHARMACEUTICAL',\n",
       "  'LABORATORIO KEMEX S.A.',\n",
       "  'LEGACY PHARMACEUTICALS',\n",
       "  'IMCD CANADA LIMITED',\n",
       "  'CHENGDU YATU BIOLOGICAL TECHNOLOGY',\n",
       "  'LES LABORATOIRES BIOTECH',\n",
       "  'ESPRI ERIKA STOLLENWERK PRIM√ÑRVERPACKUNG',\n",
       "  'MTF',\n",
       "  'AALTO SCIENTIFIC',\n",
       "  'VMIC',\n",
       "  'BIOTHEUS BIOPHARMACEUTICAL',\n",
       "  'EUROMEDICINA D.O.O',\n",
       "  'SHANGHAI MIRACOGEN INC.',\n",
       "  'CIMA TECHNOLOGY INC',\n",
       "  'ONEGENE BIOTECHNOLOGY',\n",
       "  'STREULI PHARMA AG',\n",
       "  'NAVIDEA BIOPHARMACEUTICALS, INC',\n",
       "  'NEVAKAR LLC',\n",
       "  'RHOSHAN PHARMACEUTICALS',\n",
       "  'INVITRO INTERNATIONAL',\n",
       "  'MUDANJIANG YOU BO PHARMACEUTICAL',\n",
       "  'REPRODUX LABORATORIOS LTDA',\n",
       "  'SN CORP.',\n",
       "  'WATERS TECHNOLOGIES CORPORATION',\n",
       "  'CENTURION ILAC SAN.VE TIC.A.S.',\n",
       "  'MEDALLIANCE LLC',\n",
       "  'BIOMM S/A',\n",
       "  'WRIGHT MEDICAL TECHNOLOGY, INC',\n",
       "  'IMA',\n",
       "  'AQUA SCIENCE',\n",
       "  'IMMUNOGEN INC',\n",
       "  'MODERN WATER INC',\n",
       "  'PERKIN ELMER',\n",
       "  'QUALITY BIORESOURCES, INC'],\n",
       " 'created_on': ['Thursday, 12 January 2023',\n",
       "  'Friday, 13 January 2023',\n",
       "  'Saturday, 14 January 2023',\n",
       "  'Sunday, 15 January 2023',\n",
       "  'Monday, 16 January 2023',\n",
       "  'Tuesday, 17 January 2023',\n",
       "  'Wednesday, 18 January 2023',\n",
       "  'Thursday, 19 January 2023',\n",
       "  'Friday, 20 January 2023',\n",
       "  'Saturday, 21 January 2023',\n",
       "  'Sunday, 22 January 2023',\n",
       "  'Monday, 23 January 2023',\n",
       "  'Tuesday, 24 January 2023',\n",
       "  'Wednesday, 25 January 2023',\n",
       "  'Thursday, 26 January 2023',\n",
       "  'Friday, 27 January 2023',\n",
       "  'Saturday, 28 January 2023',\n",
       "  'Sunday, 29 January 2023',\n",
       "  'Monday, 30 January 2023',\n",
       "  'Tuesday, 31 January 2023',\n",
       "  'Wednesday, 1 February 2023',\n",
       "  'Thursday, 2 February 2023',\n",
       "  'Friday, 3 February 2023',\n",
       "  'Saturday, 4 February 2023',\n",
       "  'Sunday, 5 February 2023',\n",
       "  'Monday, 6 February 2023',\n",
       "  'Tuesday, 7 February 2023',\n",
       "  'Wednesday, 8 February 2023',\n",
       "  'Thursday, 9 February 2023',\n",
       "  'Friday, 10 February 2023',\n",
       "  'Saturday, 11 February 2023',\n",
       "  'Sunday, 12 February 2023',\n",
       "  'Monday, 13 February 2023',\n",
       "  'Tuesday, 14 February 2023',\n",
       "  'Wednesday, 15 February 2023',\n",
       "  'Thursday, 16 February 2023',\n",
       "  'Friday, 17 February 2023',\n",
       "  'Saturday, 18 February 2023',\n",
       "  'Sunday, 19 February 2023',\n",
       "  'Monday, 20 February 2023',\n",
       "  'Tuesday, 21 February 2023',\n",
       "  'Wednesday, 22 February 2023',\n",
       "  'Thursday, 23 February 2023',\n",
       "  'Friday, 24 February 2023',\n",
       "  'Saturday, 25 February 2023',\n",
       "  'Sunday, 26 February 2023',\n",
       "  'Monday, 27 February 2023',\n",
       "  'Tuesday, 28 February 2023',\n",
       "  'Wednesday, 1 March 2023',\n",
       "  'Thursday, 2 March 2023',\n",
       "  'Friday, 3 March 2023',\n",
       "  'Saturday, 4 March 2023',\n",
       "  'Sunday, 5 March 2023',\n",
       "  'Monday, 6 March 2023',\n",
       "  'Tuesday, 7 March 2023',\n",
       "  'Wednesday, 8 March 2023',\n",
       "  'Thursday, 9 March 2023',\n",
       "  'Friday, 10 March 2023',\n",
       "  'Saturday, 11 March 2023',\n",
       "  'Sunday, 12 March 2023',\n",
       "  'Monday, 13 March 2023',\n",
       "  'Tuesday, 14 March 2023',\n",
       "  'Wednesday, 15 March 2023',\n",
       "  'Thursday, 16 March 2023',\n",
       "  'Friday, 17 March 2023',\n",
       "  'Saturday, 18 March 2023',\n",
       "  'Sunday, 19 March 2023',\n",
       "  'Monday, 20 March 2023',\n",
       "  'Tuesday, 21 March 2023',\n",
       "  'Wednesday, 22 March 2023',\n",
       "  'Thursday, 23 March 2023',\n",
       "  'Friday, 24 March 2023',\n",
       "  'Saturday, 25 March 2023',\n",
       "  'Sunday, 26 March 2023',\n",
       "  'Monday, 27 March 2023',\n",
       "  'Tuesday, 28 March 2023',\n",
       "  'Wednesday, 29 March 2023',\n",
       "  'Thursday, 30 March 2023',\n",
       "  'Friday, 31 March 2023',\n",
       "  'Saturday, 1 April 2023',\n",
       "  'Sunday, 2 April 2023',\n",
       "  'Monday, 3 April 2023',\n",
       "  'Tuesday, 4 April 2023',\n",
       "  'Wednesday, 5 April 2023',\n",
       "  'Thursday, 6 April 2023',\n",
       "  'Friday, 7 April 2023',\n",
       "  'Saturday, 8 April 2023',\n",
       "  'Sunday, 9 April 2023',\n",
       "  'Monday, 10 April 2023',\n",
       "  'Tuesday, 11 April 2023',\n",
       "  'Wednesday, 12 April 2023',\n",
       "  'Thursday, 13 April 2023',\n",
       "  'Friday, 14 April 2023',\n",
       "  'Saturday, 15 April 2023',\n",
       "  'Sunday, 16 April 2023',\n",
       "  'Monday, 17 April 2023',\n",
       "  'Tuesday, 18 April 2023',\n",
       "  'Wednesday, 19 April 2023',\n",
       "  'Thursday, 20 April 2023',\n",
       "  'Friday, 21 April 2023',\n",
       "  'Saturday, 22 April 2023',\n",
       "  'Sunday, 23 April 2023',\n",
       "  'Monday, 24 April 2023',\n",
       "  'Tuesday, 25 April 2023',\n",
       "  'Wednesday, 26 April 2023',\n",
       "  'Thursday, 27 April 2023',\n",
       "  'Friday, 28 April 2023',\n",
       "  'Saturday, 29 April 2023',\n",
       "  'Sunday, 30 April 2023',\n",
       "  'Monday, 1 May 2023',\n",
       "  'Tuesday, 2 May 2023',\n",
       "  'Wednesday, 3 May 2023',\n",
       "  'Thursday, 4 May 2023',\n",
       "  'Friday, 5 May 2023',\n",
       "  'Saturday, 6 May 2023',\n",
       "  'Sunday, 7 May 2023',\n",
       "  'Monday, 8 May 2023',\n",
       "  'Tuesday, 9 May 2023',\n",
       "  'Wednesday, 10 May 2023',\n",
       "  'Thursday, 11 May 2023',\n",
       "  'Friday, 12 May 2023',\n",
       "  'Saturday, 13 May 2023',\n",
       "  'Sunday, 14 May 2023',\n",
       "  'Monday, 15 May 2023',\n",
       "  'Tuesday, 16 May 2023',\n",
       "  'Wednesday, 17 May 2023',\n",
       "  'Thursday, 18 May 2023',\n",
       "  'Friday, 19 May 2023',\n",
       "  'Saturday, 20 May 2023',\n",
       "  'Sunday, 21 May 2023',\n",
       "  'Monday, 22 May 2023',\n",
       "  'Tuesday, 23 May 2023',\n",
       "  'Wednesday, 24 May 2023',\n",
       "  'Thursday, 25 May 2023',\n",
       "  'Friday, 26 May 2023',\n",
       "  'Saturday, 27 May 2023',\n",
       "  'Sunday, 28 May 2023',\n",
       "  'Monday, 29 May 2023',\n",
       "  'Tuesday, 30 May 2023',\n",
       "  'Wednesday, 31 May 2023',\n",
       "  'Thursday, 1 June 2023',\n",
       "  'Friday, 2 June 2023',\n",
       "  'Saturday, 3 June 2023',\n",
       "  'Sunday, 4 June 2023',\n",
       "  'Monday, 5 June 2023',\n",
       "  'Tuesday, 6 June 2023',\n",
       "  'Wednesday, 7 June 2023',\n",
       "  'Thursday, 8 June 2023',\n",
       "  'Friday, 9 June 2023',\n",
       "  'Saturday, 10 June 2023',\n",
       "  'Sunday, 11 June 2023',\n",
       "  'Monday, 12 June 2023',\n",
       "  'Tuesday, 13 June 2023',\n",
       "  'Wednesday, 14 June 2023',\n",
       "  'Thursday, 15 June 2023',\n",
       "  'Friday, 16 June 2023',\n",
       "  'Saturday, 17 June 2023',\n",
       "  'Sunday, 18 June 2023',\n",
       "  'Monday, 19 June 2023',\n",
       "  'Tuesday, 20 June 2023',\n",
       "  'Wednesday, 21 June 2023',\n",
       "  'Thursday, 22 June 2023',\n",
       "  'Friday, 23 June 2023',\n",
       "  'Saturday, 24 June 2023',\n",
       "  'Sunday, 25 June 2023',\n",
       "  'Monday, 26 June 2023',\n",
       "  'Tuesday, 27 June 2023',\n",
       "  'Wednesday, 28 June 2023',\n",
       "  'Thursday, 29 June 2023',\n",
       "  'Friday, 30 June 2023',\n",
       "  'Saturday, 1 July 2023',\n",
       "  'Sunday, 2 July 2023',\n",
       "  'Monday, 3 July 2023',\n",
       "  'Tuesday, 4 July 2023',\n",
       "  'Wednesday, 5 July 2023',\n",
       "  'Thursday, 6 July 2023',\n",
       "  'Friday, 7 July 2023',\n",
       "  'Saturday, 8 July 2023',\n",
       "  'Sunday, 9 July 2023',\n",
       "  'Monday, 10 July 2023',\n",
       "  'Tuesday, 11 July 2023',\n",
       "  'Wednesday, 12 July 2023',\n",
       "  'Thursday, 13 July 2023',\n",
       "  'Friday, 14 July 2023',\n",
       "  'Saturday, 15 July 2023',\n",
       "  'Sunday, 16 July 2023',\n",
       "  'Monday, 17 July 2023',\n",
       "  'Tuesday, 18 July 2023',\n",
       "  'Wednesday, 19 July 2023',\n",
       "  'Thursday, 20 July 2023',\n",
       "  'Friday, 21 July 2023',\n",
       "  'Saturday, 22 July 2023',\n",
       "  'Sunday, 23 July 2023',\n",
       "  'Monday, 24 July 2023',\n",
       "  'Tuesday, 25 July 2023',\n",
       "  'Wednesday, 26 July 2023',\n",
       "  'Thursday, 27 July 2023',\n",
       "  'Friday, 28 July 2023',\n",
       "  'Saturday, 29 July 2023',\n",
       "  'Sunday, 30 July 2023',\n",
       "  'Monday, 31 July 2023',\n",
       "  'Tuesday, 1 August 2023',\n",
       "  'Wednesday, 2 August 2023',\n",
       "  'Thursday, 3 August 2023',\n",
       "  'Friday, 4 August 2023',\n",
       "  'Saturday, 5 August 2023',\n",
       "  'Sunday, 6 August 2023',\n",
       "  'Monday, 7 August 2023',\n",
       "  'Tuesday, 8 August 2023',\n",
       "  'Wednesday, 9 August 2023',\n",
       "  'Thursday, 10 August 2023',\n",
       "  'Friday, 11 August 2023',\n",
       "  'Saturday, 12 August 2023',\n",
       "  'Sunday, 13 August 2023',\n",
       "  'Monday, 14 August 2023',\n",
       "  'Tuesday, 15 August 2023',\n",
       "  'Wednesday, 16 August 2023',\n",
       "  'Thursday, 17 August 2023',\n",
       "  'Friday, 18 August 2023',\n",
       "  'Saturday, 19 August 2023',\n",
       "  'Sunday, 20 August 2023',\n",
       "  'Monday, 21 August 2023',\n",
       "  'Tuesday, 22 August 2023',\n",
       "  'Wednesday, 23 August 2023',\n",
       "  'Thursday, 24 August 2023',\n",
       "  'Friday, 25 August 2023',\n",
       "  'Saturday, 26 August 2023',\n",
       "  'Sunday, 27 August 2023',\n",
       "  'Monday, 28 August 2023',\n",
       "  'Tuesday, 29 August 2023',\n",
       "  'Wednesday, 30 August 2023',\n",
       "  'Thursday, 31 August 2023',\n",
       "  'Friday, 1 September 2023',\n",
       "  'Saturday, 2 September 2023',\n",
       "  'Sunday, 3 September 2023',\n",
       "  'Monday, 4 September 2023',\n",
       "  'Tuesday, 5 September 2023',\n",
       "  'Wednesday, 6 September 2023',\n",
       "  'Thursday, 7 September 2023',\n",
       "  'Friday, 8 September 2023',\n",
       "  'Saturday, 9 September 2023',\n",
       "  'Sunday, 10 September 2023',\n",
       "  'Monday, 11 September 2023',\n",
       "  'Tuesday, 12 September 2023',\n",
       "  'Wednesday, 13 September 2023',\n",
       "  'Thursday, 14 September 2023',\n",
       "  'Friday, 15 September 2023',\n",
       "  'Saturday, 16 September 2023',\n",
       "  'Sunday, 17 September 2023',\n",
       "  'Monday, 18 September 2023',\n",
       "  'Tuesday, 19 September 2023',\n",
       "  'Wednesday, 20 September 2023',\n",
       "  'Thursday, 21 September 2023',\n",
       "  'Friday, 22 September 2023',\n",
       "  'Saturday, 23 September 2023',\n",
       "  'Sunday, 24 September 2023',\n",
       "  'Monday, 25 September 2023',\n",
       "  'Tuesday, 26 September 2023',\n",
       "  'Wednesday, 27 September 2023',\n",
       "  'Thursday, 28 September 2023',\n",
       "  'Friday, 29 September 2023',\n",
       "  'Saturday, 30 September 2023',\n",
       "  'Sunday, 1 October 2023',\n",
       "  'Monday, 2 October 2023',\n",
       "  'Tuesday, 3 October 2023',\n",
       "  'Wednesday, 4 October 2023',\n",
       "  'Thursday, 5 October 2023',\n",
       "  'Friday, 6 October 2023',\n",
       "  'Saturday, 7 October 2023',\n",
       "  'Sunday, 8 October 2023',\n",
       "  'Monday, 9 October 2023',\n",
       "  'Tuesday, 10 October 2023',\n",
       "  'Wednesday, 11 October 2023',\n",
       "  'Thursday, 12 October 2023',\n",
       "  'Friday, 13 October 2023',\n",
       "  'Saturday, 14 October 2023',\n",
       "  'Sunday, 15 October 2023',\n",
       "  'Monday, 16 October 2023',\n",
       "  'Tuesday, 17 October 2023',\n",
       "  'Wednesday, 18 October 2023',\n",
       "  'Thursday, 19 October 2023',\n",
       "  'Friday, 20 October 2023',\n",
       "  'Saturday, 21 October 2023',\n",
       "  'Sunday, 22 October 2023',\n",
       "  'Monday, 23 October 2023',\n",
       "  'Tuesday, 24 October 2023',\n",
       "  'Wednesday, 25 October 2023',\n",
       "  'Thursday, 26 October 2023',\n",
       "  'Friday, 27 October 2023',\n",
       "  'Saturday, 28 October 2023',\n",
       "  'Sunday, 29 October 2023',\n",
       "  'Monday, 30 October 2023',\n",
       "  'Tuesday, 31 October 2023',\n",
       "  'Wednesday, 1 November 2023',\n",
       "  'Thursday, 2 November 2023',\n",
       "  'Friday, 3 November 2023',\n",
       "  'Saturday, 4 November 2023',\n",
       "  'Sunday, 5 November 2023',\n",
       "  'Monday, 6 November 2023',\n",
       "  'Tuesday, 7 November 2023',\n",
       "  'Wednesday, 8 November 2023',\n",
       "  'Thursday, 9 November 2023',\n",
       "  'Friday, 10 November 2023',\n",
       "  'Saturday, 11 November 2023',\n",
       "  'Sunday, 12 November 2023',\n",
       "  'Monday, 13 November 2023',\n",
       "  'Tuesday, 14 November 2023',\n",
       "  'Wednesday, 15 November 2023',\n",
       "  'Thursday, 16 November 2023',\n",
       "  'Friday, 17 November 2023',\n",
       "  'Saturday, 18 November 2023',\n",
       "  'Sunday, 19 November 2023',\n",
       "  'Monday, 20 November 2023',\n",
       "  'Tuesday, 21 November 2023',\n",
       "  'Wednesday, 22 November 2023',\n",
       "  'Thursday, 23 November 2023',\n",
       "  'Friday, 24 November 2023',\n",
       "  'Saturday, 25 November 2023',\n",
       "  'Sunday, 26 November 2023',\n",
       "  'Monday, 27 November 2023',\n",
       "  'Tuesday, 28 November 2023',\n",
       "  'Wednesday, 29 November 2023',\n",
       "  'Thursday, 30 November 2023',\n",
       "  'Friday, 1 December 2023',\n",
       "  'Saturday, 2 December 2023',\n",
       "  'Sunday, 3 December 2023',\n",
       "  'Monday, 4 December 2023',\n",
       "  'Tuesday, 5 December 2023',\n",
       "  'Wednesday, 6 December 2023',\n",
       "  'Thursday, 7 December 2023',\n",
       "  'Friday, 8 December 2023',\n",
       "  'Saturday, 9 December 2023',\n",
       "  'Sunday, 10 December 2023',\n",
       "  'Monday, 11 December 2023',\n",
       "  'Tuesday, 12 December 2023',\n",
       "  'Wednesday, 13 December 2023',\n",
       "  'Thursday, 14 December 2023',\n",
       "  'Friday, 15 December 2023',\n",
       "  'Saturday, 16 December 2023',\n",
       "  'Sunday, 17 December 2023',\n",
       "  'Monday, 18 December 2023',\n",
       "  'Tuesday, 19 December 2023',\n",
       "  'Wednesday, 20 December 2023',\n",
       "  'Thursday, 21 December 2023',\n",
       "  'Friday, 22 December 2023',\n",
       "  'Saturday, 23 December 2023',\n",
       "  'Sunday, 24 December 2023',\n",
       "  'Monday, 25 December 2023',\n",
       "  'Tuesday, 26 December 2023',\n",
       "  'Wednesday, 27 December 2023',\n",
       "  'Thursday, 28 December 2023',\n",
       "  'Friday, 29 December 2023',\n",
       "  'Saturday, 30 December 2023',\n",
       "  'Sunday, 31 December 2023',\n",
       "  'Monday, 1 January 2024',\n",
       "  'Tuesday, 2 January 2024',\n",
       "  'Wednesday, 3 January 2024',\n",
       "  'Thursday, 4 January 2024',\n",
       "  'Friday, 5 January 2024',\n",
       "  'Saturday, 6 January 2024',\n",
       "  'Sunday, 7 January 2024',\n",
       "  'Monday, 8 January 2024',\n",
       "  'Tuesday, 9 January 2024',\n",
       "  'Wednesday, 10 January 2024',\n",
       "  'Thursday, 11 January 2024',\n",
       "  'Friday, 12 January 2024',\n",
       "  'Saturday, 13 January 2024',\n",
       "  'Sunday, 14 January 2024',\n",
       "  'Monday, 15 January 2024',\n",
       "  'Tuesday, 16 January 2024',\n",
       "  'Wednesday, 17 January 2024',\n",
       "  'Thursday, 18 January 2024',\n",
       "  'Friday, 19 January 2024',\n",
       "  'Saturday, 20 January 2024',\n",
       "  'Sunday, 21 January 2024',\n",
       "  'Monday, 22 January 2024',\n",
       "  'Tuesday, 23 January 2024',\n",
       "  'Wednesday, 24 January 2024',\n",
       "  'Thursday, 25 January 2024',\n",
       "  'Friday, 26 January 2024',\n",
       "  'Saturday, 27 January 2024',\n",
       "  'Sunday, 28 January 2024',\n",
       "  'Monday, 29 January 2024',\n",
       "  'Tuesday, 30 January 2024',\n",
       "  'Wednesday, 31 January 2024',\n",
       "  'Thursday, 1 February 2024',\n",
       "  'Friday, 2 February 2024',\n",
       "  'Saturday, 3 February 2024',\n",
       "  'Sunday, 4 February 2024',\n",
       "  'Monday, 5 February 2024',\n",
       "  'Tuesday, 6 February 2024',\n",
       "  'Wednesday, 7 February 2024',\n",
       "  'Thursday, 8 February 2024',\n",
       "  'Friday, 9 February 2024',\n",
       "  'Saturday, 10 February 2024',\n",
       "  'Sunday, 11 February 2024',\n",
       "  'Monday, 12 February 2024',\n",
       "  'Tuesday, 13 February 2024',\n",
       "  'Wednesday, 14 February 2024',\n",
       "  'Thursday, 15 February 2024',\n",
       "  'Friday, 16 February 2024',\n",
       "  'Saturday, 17 February 2024',\n",
       "  'Sunday, 18 February 2024',\n",
       "  'Monday, 19 February 2024',\n",
       "  'Tuesday, 20 February 2024',\n",
       "  'Wednesday, 21 February 2024',\n",
       "  'Thursday, 22 February 2024',\n",
       "  'Friday, 23 February 2024',\n",
       "  'Saturday, 24 February 2024',\n",
       "  'Sunday, 25 February 2024',\n",
       "  'Monday, 26 February 2024',\n",
       "  'Tuesday, 27 February 2024',\n",
       "  'Wednesday, 28 February 2024',\n",
       "  'Thursday, 29 February 2024',\n",
       "  'Friday, 1 March 2024',\n",
       "  'Saturday, 2 March 2024',\n",
       "  'Sunday, 3 March 2024',\n",
       "  'Monday, 4 March 2024',\n",
       "  'Tuesday, 5 March 2024',\n",
       "  'Wednesday, 6 March 2024',\n",
       "  'Thursday, 7 March 2024',\n",
       "  'Friday, 8 March 2024',\n",
       "  'Saturday, 9 March 2024',\n",
       "  'Sunday, 10 March 2024',\n",
       "  'Monday, 11 March 2024',\n",
       "  'Tuesday, 12 March 2024',\n",
       "  'Wednesday, 13 March 2024',\n",
       "  'Thursday, 14 March 2024',\n",
       "  'Friday, 15 March 2024',\n",
       "  'Saturday, 16 March 2024',\n",
       "  'Sunday, 17 March 2024',\n",
       "  'Monday, 18 March 2024',\n",
       "  'Tuesday, 19 March 2024',\n",
       "  'Wednesday, 20 March 2024',\n",
       "  'Thursday, 21 March 2024',\n",
       "  'Friday, 22 March 2024',\n",
       "  'Saturday, 23 March 2024',\n",
       "  'Sunday, 24 March 2024',\n",
       "  'Monday, 25 March 2024',\n",
       "  'Tuesday, 26 March 2024',\n",
       "  'Wednesday, 27 March 2024',\n",
       "  'Thursday, 28 March 2024',\n",
       "  'Friday, 29 March 2024',\n",
       "  'Saturday, 30 March 2024',\n",
       "  'Sunday, 31 March 2024',\n",
       "  'Monday, 1 April 2024',\n",
       "  'Tuesday, 2 April 2024',\n",
       "  'Wednesday, 3 April 2024',\n",
       "  'Thursday, 4 April 2024',\n",
       "  'Friday, 5 April 2024',\n",
       "  'Saturday, 6 April 2024',\n",
       "  'Sunday, 7 April 2024',\n",
       "  'Monday, 8 April 2024',\n",
       "  'Tuesday, 9 April 2024',\n",
       "  'Wednesday, 10 April 2024',\n",
       "  'Thursday, 11 April 2024',\n",
       "  'Friday, 12 April 2024',\n",
       "  'Saturday, 13 April 2024',\n",
       "  'Sunday, 14 April 2024',\n",
       "  'Monday, 15 April 2024',\n",
       "  'Tuesday, 16 April 2024',\n",
       "  'Wednesday, 17 April 2024',\n",
       "  'Thursday, 18 April 2024',\n",
       "  'Friday, 19 April 2024',\n",
       "  'Saturday, 20 April 2024',\n",
       "  'Sunday, 21 April 2024',\n",
       "  'Monday, 22 April 2024',\n",
       "  'Tuesday, 23 April 2024',\n",
       "  'Wednesday, 24 April 2024',\n",
       "  'Thursday, 25 April 2024',\n",
       "  'Friday, 26 April 2024',\n",
       "  'Saturday, 27 April 2024',\n",
       "  'Sunday, 28 April 2024',\n",
       "  'Monday, 29 April 2024',\n",
       "  'Tuesday, 30 April 2024',\n",
       "  'Wednesday, 1 May 2024',\n",
       "  'Thursday, 2 May 2024',\n",
       "  'Friday, 3 May 2024',\n",
       "  'Saturday, 4 May 2024',\n",
       "  'Sunday, 5 May 2024',\n",
       "  'Monday, 6 May 2024',\n",
       "  'Tuesday, 7 May 2024',\n",
       "  'Wednesday, 8 May 2024',\n",
       "  'Thursday, 9 May 2024',\n",
       "  'Friday, 10 May 2024',\n",
       "  'Saturday, 11 May 2024',\n",
       "  'Sunday, 12 May 2024',\n",
       "  'Monday, 13 May 2024',\n",
       "  'Tuesday, 14 May 2024',\n",
       "  'Wednesday, 15 May 2024',\n",
       "  'Thursday, 16 May 2024',\n",
       "  'Friday, 17 May 2024',\n",
       "  'Saturday, 18 May 2024',\n",
       "  'Sunday, 19 May 2024',\n",
       "  'Monday, 20 May 2024',\n",
       "  'Tuesday, 21 May 2024',\n",
       "  'Wednesday, 22 May 2024',\n",
       "  'Thursday, 23 May 2024',\n",
       "  'Friday, 24 May 2024',\n",
       "  'Saturday, 25 May 2024',\n",
       "  'Sunday, 26 May 2024',\n",
       "  'Monday, 27 May 2024',\n",
       "  'Tuesday, 28 May 2024',\n",
       "  'Wednesday, 29 May 2024',\n",
       "  'Thursday, 30 May 2024',\n",
       "  'Friday, 31 May 2024',\n",
       "  'Saturday, 1 June 2024',\n",
       "  'Sunday, 2 June 2024',\n",
       "  'Monday, 3 June 2024',\n",
       "  'Tuesday, 4 June 2024',\n",
       "  'Wednesday, 5 June 2024',\n",
       "  'Thursday, 6 June 2024',\n",
       "  'Friday, 7 June 2024',\n",
       "  'Saturday, 8 June 2024',\n",
       "  'Sunday, 9 June 2024',\n",
       "  'Monday, 10 June 2024',\n",
       "  'Tuesday, 11 June 2024',\n",
       "  'Wednesday, 12 June 2024',\n",
       "  'Thursday, 13 June 2024',\n",
       "  'Friday, 14 June 2024',\n",
       "  'Saturday, 15 June 2024',\n",
       "  'Sunday, 16 June 2024',\n",
       "  'Monday, 17 June 2024',\n",
       "  'Tuesday, 18 June 2024',\n",
       "  'Wednesday, 19 June 2024',\n",
       "  'Thursday, 20 June 2024',\n",
       "  'Friday, 21 June 2024',\n",
       "  'Saturday, 22 June 2024',\n",
       "  'Sunday, 23 June 2024',\n",
       "  'Monday, 24 June 2024',\n",
       "  'Tuesday, 25 June 2024',\n",
       "  'Wednesday, 26 June 2024',\n",
       "  'Thursday, 27 June 2024',\n",
       "  'Friday, 28 June 2024',\n",
       "  'Saturday, 29 June 2024',\n",
       "  'Sunday, 30 June 2024',\n",
       "  'Monday, 1 July 2024',\n",
       "  'Tuesday, 2 July 2024',\n",
       "  'Wednesday, 3 July 2024',\n",
       "  'Thursday, 4 July 2024',\n",
       "  'Friday, 5 July 2024',\n",
       "  'Saturday, 6 July 2024',\n",
       "  'Sunday, 7 July 2024',\n",
       "  'Monday, 8 July 2024',\n",
       "  'Tuesday, 9 July 2024',\n",
       "  'Wednesday, 10 July 2024',\n",
       "  'Thursday, 11 July 2024',\n",
       "  'Friday, 12 July 2024',\n",
       "  'Saturday, 13 July 2024',\n",
       "  'Sunday, 14 July 2024',\n",
       "  'Monday, 15 July 2024',\n",
       "  'Tuesday, 16 July 2024',\n",
       "  'Wednesday, 17 July 2024',\n",
       "  'Thursday, 18 July 2024',\n",
       "  'Friday, 19 July 2024',\n",
       "  'Saturday, 20 July 2024',\n",
       "  'Sunday, 21 July 2024',\n",
       "  'Monday, 22 July 2024',\n",
       "  'Tuesday, 23 July 2024',\n",
       "  'Wednesday, 24 July 2024',\n",
       "  'Thursday, 25 July 2024',\n",
       "  'Friday, 26 July 2024',\n",
       "  'Saturday, 27 July 2024',\n",
       "  'Sunday, 28 July 2024',\n",
       "  'Monday, 29 July 2024',\n",
       "  'Tuesday, 30 July 2024',\n",
       "  'Wednesday, 31 July 2024',\n",
       "  'Thursday, 1 August 2024',\n",
       "  'Friday, 2 August 2024',\n",
       "  'Saturday, 3 August 2024',\n",
       "  'Sunday, 4 August 2024',\n",
       "  'Monday, 5 August 2024',\n",
       "  'Tuesday, 6 August 2024',\n",
       "  'Wednesday, 7 August 2024',\n",
       "  'Thursday, 8 August 2024',\n",
       "  'Friday, 9 August 2024',\n",
       "  'Saturday, 10 August 2024',\n",
       "  'Sunday, 11 August 2024',\n",
       "  'Monday, 12 August 2024',\n",
       "  'Tuesday, 13 August 2024']}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_cat_cols(data_types: dict) -> list[str]:\n",
    "    \"\"\"Function to return all the categorical column names from a schema dictionary\n",
    "\n",
    "    Args:\n",
    "        data_types (dict): A dictionary containing column names as keys and their corresponding data types as values\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of categorical column names in the input schema dictionary\n",
    "    \"\"\"\n",
    "\n",
    "    # Create an empty list to store the categorical column names\n",
    "    cat_cols = []\n",
    "\n",
    "    # Iterate over each column in the input schema dictionary and check if it is a categorical column or not\n",
    "    for col, dtype in data_types.items():\n",
    "        if dtype == 'object':\n",
    "            cat_cols.append(col)\n",
    "        elif dtype.name == 'category':\n",
    "            cat_cols.append(col)\n",
    "        # Add more conditions to check if a data type is categorical or not depending on the use case\n",
    "\n",
    "    # Return the list of categorical column names\n",
    "    return cat_cols\n",
    "\n",
    "# Generating a dictionary having all unique values of all the categorical columns\n",
    "categorical_columns = get_unique_values(df=df, cat_cols=get_cat_cols(data_types=data_type))\n",
    "categorical_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synonyms\n",
    "- Simple implementation of plural variations\n",
    "- TODO: Generating relevant variation of synonyms using llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "camel Case Example\n",
      "Pascal Case Example\n",
      "snake case example\n",
      "Unit Price\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def split_and_join_string(s):\n",
    "    # Splitting snake_case\n",
    "    if '_' in s:\n",
    "        split_words = s.split('_')\n",
    "    \n",
    "    # Splitting camelCase and PascalCase\n",
    "    else:\n",
    "        split_words = re.findall('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', s)\n",
    "    \n",
    "    # Joining the split words with a space\n",
    "    return ' '.join(split_words)\n",
    "\n",
    "# Examples\n",
    "print(split_and_join_string(\"camelCaseExample\"))  # camelCase\n",
    "print(split_and_join_string(\"PascalCaseExample\")) # PascalCase\n",
    "print(split_and_join_string(\"snake_case_example\")) # snake_case\n",
    "print(split_and_join_string(\"UnitPrice\")) # snake_case\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synonym_dict(data_types: dict, generate_synonyms: bool = False) -> dict:\n",
    "    \"\"\"\n",
    "    Creates a dictionary with keys as Attributes and their plural forms as basic synonyms.\n",
    "\n",
    "    Args:\n",
    "         data_types: A dictionary containing attribute names as keys.\n",
    "\n",
    "    Returns: \n",
    "        A dictionary with attribute names as keys and their plural forms as values.\n",
    "    \"\"\"\n",
    "    \n",
    "    synonyms = {}\n",
    "    for column_name in data_types.keys():\n",
    "\n",
    "        # Split and create a single word in case of camelCase, PascalCase or snake_case for better search results\n",
    "        split_col_name = split_and_join_string(column_name)\n",
    "        synonyms[column_name] = [split_col_name]\n",
    "\n",
    "        if column_name[-1] == \"y\":\n",
    "            synonyms[column_name].append(f\"{split_col_name[:-1]}ies\")\n",
    "        elif column_name[-1] in (\"s\", \"x\"):\n",
    "            synonyms[column_name].append(f\"{split_col_name[:-1]}es\")\n",
    "        else:\n",
    "            synonyms[column_name].append(f\"{split_col_name}s\")\n",
    "        \n",
    "        # Appending generated synonyms\n",
    "        if generate_synonyms:\n",
    "            try:\n",
    "                synonyms[column_name].extend(eval(generate_prompt.generate_synonyms([split_col_name])))\n",
    "                # In case LLM output is not a list\n",
    "            except Exception as e:\n",
    "                print(f\"Error in Generating Synonyms: {e}\")\n",
    "                synonyms[column_name].extend(generate_prompt.generate_synonyms([split_col_name]))\n",
    "\n",
    "    return synonyms\n",
    "\n",
    "synonyms = create_synonym_dict(data_types=data_type, generate_synonyms=True)\n",
    "synonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persist Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb_dict = {'data_type':data_type,'categorical_columns':categorical_columns,'synonyms':synonyms}\n",
    "with open(f\"{file_name}.pkl\", 'wb') as f:\n",
    "    pickle.dump(kb_dict, f)\n",
    "\n",
    "# # Persist in JSON\n",
    "# with open(f'{file_name}.json', 'w') as f:\n",
    "#     json.dump(kb_dict, f , indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{file_name}.pkl\", 'rb') as f:\n",
    "    schema_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing Knowledgebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column_name: 'Group', 'object'\n",
      "synonyms: ['Group', 'Groups', 'Team', 'Class', 'Cluster', 'Cohort', 'Squad', 'Section', 'Division', 'Guild', 'Circle', 'Collective']\n",
      "unique_values: ['Core Products', 'HVP']\n",
      "column_name: 'GLOBAL CUSTOMER FULL', 'object'\n",
      "synonyms: ['GLOBAL CUSTOMER FULL', 'GLOBAL CUSTOMER FULLs', 'Client', 'Consumer', 'Patron', 'User', 'CustomerBase', 'Market', 'Audience', 'Demographic', 'Public', 'Community']\n",
      "unique_values: ['NOVO NORDISK AS', 'INIBSA', 'YICHANG RENFU MEDICINE', 'BAXTER INTERNATIONAL', 'BECTON DICKINSON & COMPANY', 'HUONS - KOREA', 'B.BRAUN MELSUNGEN AG', 'REIG JOFRE GROUP', 'JSC FARMAK', 'SQUARE PHARMACEUTICALS LIMITED', 'LABORATORIO P. CASSARA S.R.L.', 'LYOCONTRACT - GLOBAL', 'ISO-TEX DIAGNOSTICS INC', 'VALDEPHARM', 'DWK LIFE SCIENCES LIMITED', 'TRICORBRAUN', 'APSEN FARMACEUTICA S/A', 'SMALL VOLUME - CORE', 'LG CHEMICAL - KOREA', 'EUROFARMA', 'GEN ILAC VE SAGLIK √úR√úNLERI AS.', 'SINTETICA SA', 'CURIUM US LLC', 'NEON LABORATORIES LTD', 'ZOETIS', 'SIEMENS AG', 'PT SINAR GOLDSINDO', 'EVAPHARMA FOR PHARMACEUTICAL', 'G.S. COSMECEUTICAL USA INC', 'CARDINAL HEALTH', 'INDUSTRIAL VETERINARIA, S.A.', 'GE HEALTHCARE', 'WPS AFFILIATES', 'ALCON RESEARCH LLC', 'GRIFOLS INC', 'STELIS BIOPHARMA LIMITED', 'NCPC BIOTECH SUPPLIES BRANCH', 'EMAD TRADE HOUSE FZCO', 'RECIPHARM', 'FERRING PHARMACEUTICALS', 'INCEPTA PHARMA - GLOBAL', 'STALLERGENES SAS', 'DONGKOOK PHARMACEUTICAL', 'BIOTEST', 'DONG-A ST CHEONAN PLANT', 'MALLINCKRODT', 'BELL-MORE LABS INC', 'CEVA ANIMAL HEALTH PTY LTD', 'SOLID PHASE INC', 'LABORATORIOS SYVA, S.A', 'FAMAR HEALTH CARE SERVICES MADRID S.A.U.', 'BIO-RAD', 'CHIA TAI TIANQING PHARMACEUTICAL GR', 'BIOMED CO LTD', 'REVALESIO CORPORATION', 'HANHUI PHARMACEUTICALS CO., LTD.', 'JULIO FORNARO S.A.', 'MS PHARMA/JORDAN', 'ALTAN PHARMACEUTICALS S.A.U.', 'GUIZHOU JINGFENG INJECTION', 'CRISTALIA', 'CSPC BAIKE', 'OOO PHARMTECHSERVICE', 'RELIANCE LIFE SCIENCES PVT. LTD', 'CSPC REFLOW PHARMACEUTICAL', 'HILTON PHARMA', 'POLIFARMA ILAC SAN. VE TIC. A.S', 'KHANDELWAL LAB. PVT. LTD', 'AMPHASTAR PHARMACEUTICALS', 'GLOBAL MEDICAL SOLUTIONS PL', 'BEXIMCO PHARMACEUTICALS LIMITED', 'WOCKHARDT LTD', 'SHANGHAI JINGFENG PHARMA', 'AJPHARMCOPUNCTURE', 'LABORATORIO LKM S.A.', 'PALLEON PHARMACEUTICALS, INC', 'DELPHARM SAINT REMY SAS', 'ISDO, S.L.', 'GREER LABORATORIES INC', 'KLOSTERFRAU HEALTHCARE GROUP', 'MAVLAB ANIMAL HEALTH PTY LTD', 'GETWELL PHARMACEUTICALS', 'HEMOFARM A.D.', 'M/S. JULPHAR, GULF PHARMACEUTICAL', 'EUBIOLOGICS CO., LTD.', 'G.L .PHARMA GMBH', 'CADILA PHARMACEUTICALS LTD', 'HIPRA', 'ONKO', 'BHARAT SERUMS & VACCINES', 'MP BIOMEDICALS LLC', 'FACET ANALYTICAL SERVICES', 'TECNOVAX S.A.', 'CREOSALUS INC', 'LABORATORIOS NORMON SA', 'HEALTHCARE PHARMACEUTICALS LTD.', 'GERRESHEIMER BUNDE', 'DIASORIN INC', 'HANLIM PHARM', 'BIOMERIEUX - GLOBAL', 'SEPTODONT/NOVOCOL', 'ZHEJIANG HUAHAI BIOTECHNOLOGY', 'VITAMED D.O.O.', 'REVOLUTION COSMETICS USA PTE LTD', 'ZSCHEILE & KLINGER GMBH', 'BERLIN PACKAGING', 'SOTHEMA', 'BMIKOREA', 'PROMEGA', 'SHENZHEN TECHDOW PHARMACEUTICA', 'SOLUPHARM PHARMA - GLOBAL', 'AENOVA GROUP', 'ANHUI ANKE BIOTECHNOLOGY', 'ADVANCED CHEMICAL INDUSTRIES LTD', 'PHARMEDIPACKDIREKT', 'AKESO BIOPHARMA, INC', 'HANGZHOU ZHONGMEI HUADONG', 'ARISTOPHARMA LIMITED', 'APODANNORDIC PHARMAPACKAGING A', 'SALUTE VITA FARMACIA DE MANIPULACAO LTDA', 'KOREA UNITED PHARM. INC.', 'NORDIC PACK AB', 'OSEL ILAC SANAYI VE TICARET AS', 'ELANCO', 'ALK-ABELLO, S.A.', 'HARALD MARKMANN', 'ZHAOKE PHARMACEUTICAL', 'SHENZHEN JIANXIANG BIOPHARMACEUTICAL', \"ST. JUDE CHILDREN'S RESEARCH\", 'CHENGDU GINEBIO BIOLOGICAL TECHNOLOGY', 'FIDIA FARMACEUTICI SPA', 'LEIDOS BIOMEDICAL RESEARCH INC', 'LAB. DE PRODUCTOS ETICOS CEISA', 'CONTACARE OPTHALMICS & DIAGNOSTICS', 'SHANGHAI YUAN LAI TRADING CO.,LTD', 'CKD BIO CORP.', 'VHG LABS INC', 'VIRCHOW BIOTECH PVT LTD', 'WEGO GROUP', 'ABBOTT', 'TRIRX SEGRE SAS', 'FUJIREBIO DIAGNOSTICS INC', 'LES LABORATOIRES MEDIS S.A.', 'MARCHESINI GROUP SPA', 'GROUPE PARIMA INC', 'PANACEA BIOTEC LTD', 'INJEMED MEDICAMENTOS ESPECIAIS LTDA', 'ATABAY', 'PACKAGING COORDINATORS INC', 'PROD.FARMACEUT.DR. GRAY SACI', 'RESILIENCE BIOTECHNOLOGIES INC', 'MASSACHUSETTS BIOLOGICS', 'TCOAG IRELAND LTD', 'WINTAC', 'QUIMFA S.A.', 'WORLD  MEDICINE  ILAC SAN VE TIC AS', 'SHANDONG PHARMA GLASS', 'MERIDIAN MEDICAL TECHNOLOGIES', 'HELENA LABORATORIES', 'DAIHAN PHARM. CO., LTD.', 'NORBROOK LABORATORIES LTD.', 'ZHONGSHAN SINOBIOWAY HYGENE', 'TECNONUCLEAR S.A.', 'CALIBER THERAPEUTICS', 'CRONUS PHARMA SPECIALITIES INDIA', 'BAUSCH & LOMB INCORPORATED', 'UNITED CHEMICAL TECHNOLOGIES', 'ACS DOBFAR S.P.A.', 'HUA RUN ANGDE BIOCHEMICAL', 'LABYES S.A.', 'GADOR S.A.', 'CEDARLANE', 'BIOGENESIS BAGO SA', 'QIAGEN SCIENCES LLC', 'MEDIPOST CO., LTD.', 'SCIVAC', 'LEVO THERAPEUTICS, INC', 'IVAX ARGENTINA S.A.', 'ALTASCIENCES CDMO PHILADELPHIA, LLC', 'MEFAR ILAC SAN.  A.S.', 'NEPHRON SC INC', 'ALIDAC PHARMACEUTICALS LIMITED', 'QUIDEL CORP - GLOBAL', 'LABORATORIOS CALIER, S.A.', 'ALFASIGMA S.P.A.', 'MEDICAL UNION', 'BIOLYPH LLC', 'SHANDONG NEW TIMES PHARMA', 'SUZHOU ZELGEN BIOPHARMACEUTICALS', 'SAMARTH LIFE SCIENCES PVT LTD', 'TARCHOMINSKIE ZAKLADY', 'INNOXEL LIFESCIENCES PRIVATE LIMITED', 'AUG PHARMA', 'LEEDSAY IND. E COM. DE PROD. MED. LTDA', 'LABORATORIOS AC FARMA S.A.', 'DENOVA PHARMACEUTICAL LTDA.', 'INTERNATIONAL PHARMACEUTICAL IMMUNOLOGY', 'DCE DIETETIQUE', 'IBSA INSTITUT BIOCHIMIQUE SA', 'INST. BIOLOGICO CONTEMPORANEO', 'MENARINI SAGLIK VE ILAC SAN. TIC. A.S.', 'SLAYBACK PHARMA INDIA LLP', 'ICU MEDICAL INC', 'SOPHAL SPA ALG√âRIE', 'SHANGHAI FUDAN-ZHANGJIANG', 'WAISMAN BIOMANUFACTURING', 'LUCKFURT INDUSTRIE HANDELS GMBH', 'SHANXI ZHENDONG GROUP CO., LTD.', 'DEKA RESEARCH & DEVELOPMENT CORP', 'EVIVE BIOTECHNOLGY.,LTD', 'INC PHARMA', 'YUKON MEDICAL', 'SINOMAB BIOSCIENCE ÔºàHAINANÔºâ', 'HOLLIDAY SCOTT S.A.', 'JSC PHARMSTANDARD-UFAVITA', 'BIOSIDUS S.A.', 'TECNOQUIMICAS S.A.', 'MEDI-RADIOPHARMA LTD', 'SHANGHAI SUNWAY BIOTECH', 'KOCAK FARMA - GLOBAL', 'HANA PHARM CO., LTD.', 'PACKAGING COORDINATORS, INC', 'COOPERMEDICAL SRL', 'SMITHFIELD BIOSCIENCE INC', 'MICRO MEASUREMENT LABORATORIES', 'PHARCO COOPERATION', 'MEDIFARMA S.A', 'AMERICAN INJECTABLES INC', 'SARL HUPP PHARMA', 'SICHUAN HUI YU PHARMA LTD.,CO', 'ZHEJIANG POLY PHARM. CO.,LTD.', 'VEM ILAC SAN. VE TIC. A.S.', 'NYMOX PHARMACEUTICAL CORPORATION', 'BEIJING SHUANG LU PHARMA LTD, CO.', 'AL ANDALOUS FOR PHARMACEUTICAL IND.', 'ASTORIA-PACIFIC INC', 'OYSTER POINT PHARMA', 'LG CHEM, LTD.', 'MS PHARMA ƒ∞LA√á SAN. VE Tƒ∞C. A.≈û  *OBS*', 'SHARP PACKAGING SOLUTIONS', 'BLUE WATER VACCINES', 'J.H. RITMEESTER B.V.', 'BEL-ART PRODUCTS', 'MICROSULES ARGENTINA S.A.', 'MP BIOMEDICALS NEW ZEALAND LTD', 'SL VAXIGEN INC.', 'CHROMATOGRAPHIC SPECIALTIES INC', 'TONGYI PHARMACEUTICAL', 'LABORATORIOS CASASCO S.A.I.C.', 'SHAMROCK GLASS COMPANY', 'LANZHOU BIOTECHNOLOGY', 'SHANXI ZHENDONG PHARMACEUTICAL', 'SHANGHAI UNITED CELL BIOTECHNOLOGY', 'LAFEDAR S.A.', 'MESOSYSTEM SA', 'AMRIYA PHARMACEUTICAL', 'CHENGDU BRILLIANT PHARMACEUTICAL', 'LABORATORIO KEMEX S.A.', 'LEGACY PHARMACEUTICALS', 'IMCD CANADA LIMITED', 'CHENGDU YATU BIOLOGICAL TECHNOLOGY', 'LES LABORATOIRES BIOTECH', 'ESPRI ERIKA STOLLENWERK PRIM√ÑRVERPACKUNG', 'MTF', 'AALTO SCIENTIFIC', 'VMIC', 'BIOTHEUS BIOPHARMACEUTICAL', 'EUROMEDICINA D.O.O', 'SHANGHAI MIRACOGEN INC.', 'CIMA TECHNOLOGY INC', 'ONEGENE BIOTECHNOLOGY', 'STREULI PHARMA AG', 'NAVIDEA BIOPHARMACEUTICALS, INC', 'NEVAKAR LLC', 'RHOSHAN PHARMACEUTICALS', 'INVITRO INTERNATIONAL', 'MUDANJIANG YOU BO PHARMACEUTICAL', 'REPRODUX LABORATORIOS LTDA', 'SN CORP.', 'WATERS TECHNOLOGIES CORPORATION', 'CENTURION ILAC SAN.VE TIC.A.S.', 'MEDALLIANCE LLC', 'BIOMM S/A', 'WRIGHT MEDICAL TECHNOLOGY, INC', 'IMA', 'AQUA SCIENCE', 'IMMUNOGEN INC', 'MODERN WATER INC', 'PERKIN ELMER', 'QUALITY BIORESOURCES, INC']\n",
      "column_name: 'ActualNetSales_sum', 'float64'\n",
      "synonyms: ['ActualNetSales sum', 'ActualNetSales_sums', 'Revenue', 'Sales', 'Income', 'Turnover', 'Proceeds', 'Gross_sales', 'Total_revenue', 'Realized_sales', 'Sales_volume', 'Sales_amount']\n",
      "column_name: 'ActualQty_sum', 'float64'\n",
      "synonyms: ['ActualQty sum', 'ActualQty_sums', 'Quantity', 'Amount', 'Total', 'Volume', 'Stock', 'Supply', 'Demand', 'Inventory', 'Delivery', 'Shipment']\n",
      "column_name: 'GroupContributionToOurRevenue%', 'float64'\n",
      "synonyms: ['Group Contribution To Our Revenue%', 'GroupContributionToOurRevenue%s', 'Share', 'Contrib.', 'Quota', 'Portion', 'Proportion', 'Allocation', 'Percentage', 'Contribution', 'Division', 'Segment']\n",
      "column_name: 'UnitPrice', 'float64'\n",
      "synonyms: ['Unit Price', 'UnitPrices', 'Price', 'Fee', 'Rate', 'Charge', 'Cost', 'Tariff', 'Value', 'Quota', 'Fare', 'Margin']\n",
      "column_name: 'UnitPriceDiff', 'float64'\n",
      "synonyms: ['Unit Price Diff', 'UnitPriceDiffs', 'Difference', 'Variance', 'Change', 'Disparity', 'Distinction', 'Gap', 'Spread', 'Marginal_difference', 'Fluctuation', 'Shift']\n",
      "column_name: 'created_on', 'object'\n",
      "synonyms: ['created on', 'created_ons', 'established', 'originated', 'birthed', 'commenced', 'launched', 'initiated', 'founded', 'instituted', 'introduced', 'created_at']\n",
      "unique_values: ['Thursday, 12 January 2023', 'Friday, 13 January 2023', 'Saturday, 14 January 2023', 'Sunday, 15 January 2023', 'Monday, 16 January 2023', 'Tuesday, 17 January 2023', 'Wednesday, 18 January 2023', 'Thursday, 19 January 2023', 'Friday, 20 January 2023', 'Saturday, 21 January 2023', 'Sunday, 22 January 2023', 'Monday, 23 January 2023', 'Tuesday, 24 January 2023', 'Wednesday, 25 January 2023', 'Thursday, 26 January 2023', 'Friday, 27 January 2023', 'Saturday, 28 January 2023', 'Sunday, 29 January 2023', 'Monday, 30 January 2023', 'Tuesday, 31 January 2023', 'Wednesday, 1 February 2023', 'Thursday, 2 February 2023', 'Friday, 3 February 2023', 'Saturday, 4 February 2023', 'Sunday, 5 February 2023', 'Monday, 6 February 2023', 'Tuesday, 7 February 2023', 'Wednesday, 8 February 2023', 'Thursday, 9 February 2023', 'Friday, 10 February 2023', 'Saturday, 11 February 2023', 'Sunday, 12 February 2023', 'Monday, 13 February 2023', 'Tuesday, 14 February 2023', 'Wednesday, 15 February 2023', 'Thursday, 16 February 2023', 'Friday, 17 February 2023', 'Saturday, 18 February 2023', 'Sunday, 19 February 2023', 'Monday, 20 February 2023', 'Tuesday, 21 February 2023', 'Wednesday, 22 February 2023', 'Thursday, 23 February 2023', 'Friday, 24 February 2023', 'Saturday, 25 February 2023', 'Sunday, 26 February 2023', 'Monday, 27 February 2023', 'Tuesday, 28 February 2023', 'Wednesday, 1 March 2023', 'Thursday, 2 March 2023', 'Friday, 3 March 2023', 'Saturday, 4 March 2023', 'Sunday, 5 March 2023', 'Monday, 6 March 2023', 'Tuesday, 7 March 2023', 'Wednesday, 8 March 2023', 'Thursday, 9 March 2023', 'Friday, 10 March 2023', 'Saturday, 11 March 2023', 'Sunday, 12 March 2023', 'Monday, 13 March 2023', 'Tuesday, 14 March 2023', 'Wednesday, 15 March 2023', 'Thursday, 16 March 2023', 'Friday, 17 March 2023', 'Saturday, 18 March 2023', 'Sunday, 19 March 2023', 'Monday, 20 March 2023', 'Tuesday, 21 March 2023', 'Wednesday, 22 March 2023', 'Thursday, 23 March 2023', 'Friday, 24 March 2023', 'Saturday, 25 March 2023', 'Sunday, 26 March 2023', 'Monday, 27 March 2023', 'Tuesday, 28 March 2023', 'Wednesday, 29 March 2023', 'Thursday, 30 March 2023', 'Friday, 31 March 2023', 'Saturday, 1 April 2023', 'Sunday, 2 April 2023', 'Monday, 3 April 2023', 'Tuesday, 4 April 2023', 'Wednesday, 5 April 2023', 'Thursday, 6 April 2023', 'Friday, 7 April 2023', 'Saturday, 8 April 2023', 'Sunday, 9 April 2023', 'Monday, 10 April 2023', 'Tuesday, 11 April 2023', 'Wednesday, 12 April 2023', 'Thursday, 13 April 2023', 'Friday, 14 April 2023', 'Saturday, 15 April 2023', 'Sunday, 16 April 2023', 'Monday, 17 April 2023', 'Tuesday, 18 April 2023', 'Wednesday, 19 April 2023', 'Thursday, 20 April 2023', 'Friday, 21 April 2023', 'Saturday, 22 April 2023', 'Sunday, 23 April 2023', 'Monday, 24 April 2023', 'Tuesday, 25 April 2023', 'Wednesday, 26 April 2023', 'Thursday, 27 April 2023', 'Friday, 28 April 2023', 'Saturday, 29 April 2023', 'Sunday, 30 April 2023', 'Monday, 1 May 2023', 'Tuesday, 2 May 2023', 'Wednesday, 3 May 2023', 'Thursday, 4 May 2023', 'Friday, 5 May 2023', 'Saturday, 6 May 2023', 'Sunday, 7 May 2023', 'Monday, 8 May 2023', 'Tuesday, 9 May 2023', 'Wednesday, 10 May 2023', 'Thursday, 11 May 2023', 'Friday, 12 May 2023', 'Saturday, 13 May 2023', 'Sunday, 14 May 2023', 'Monday, 15 May 2023', 'Tuesday, 16 May 2023', 'Wednesday, 17 May 2023', 'Thursday, 18 May 2023', 'Friday, 19 May 2023', 'Saturday, 20 May 2023', 'Sunday, 21 May 2023', 'Monday, 22 May 2023', 'Tuesday, 23 May 2023', 'Wednesday, 24 May 2023', 'Thursday, 25 May 2023', 'Friday, 26 May 2023', 'Saturday, 27 May 2023', 'Sunday, 28 May 2023', 'Monday, 29 May 2023', 'Tuesday, 30 May 2023', 'Wednesday, 31 May 2023', 'Thursday, 1 June 2023', 'Friday, 2 June 2023', 'Saturday, 3 June 2023', 'Sunday, 4 June 2023', 'Monday, 5 June 2023', 'Tuesday, 6 June 2023', 'Wednesday, 7 June 2023', 'Thursday, 8 June 2023', 'Friday, 9 June 2023', 'Saturday, 10 June 2023', 'Sunday, 11 June 2023', 'Monday, 12 June 2023', 'Tuesday, 13 June 2023', 'Wednesday, 14 June 2023', 'Thursday, 15 June 2023', 'Friday, 16 June 2023', 'Saturday, 17 June 2023', 'Sunday, 18 June 2023', 'Monday, 19 June 2023', 'Tuesday, 20 June 2023', 'Wednesday, 21 June 2023', 'Thursday, 22 June 2023', 'Friday, 23 June 2023', 'Saturday, 24 June 2023', 'Sunday, 25 June 2023', 'Monday, 26 June 2023', 'Tuesday, 27 June 2023', 'Wednesday, 28 June 2023', 'Thursday, 29 June 2023', 'Friday, 30 June 2023', 'Saturday, 1 July 2023', 'Sunday, 2 July 2023', 'Monday, 3 July 2023', 'Tuesday, 4 July 2023', 'Wednesday, 5 July 2023', 'Thursday, 6 July 2023', 'Friday, 7 July 2023', 'Saturday, 8 July 2023', 'Sunday, 9 July 2023', 'Monday, 10 July 2023', 'Tuesday, 11 July 2023', 'Wednesday, 12 July 2023', 'Thursday, 13 July 2023', 'Friday, 14 July 2023', 'Saturday, 15 July 2023', 'Sunday, 16 July 2023', 'Monday, 17 July 2023', 'Tuesday, 18 July 2023', 'Wednesday, 19 July 2023', 'Thursday, 20 July 2023', 'Friday, 21 July 2023', 'Saturday, 22 July 2023', 'Sunday, 23 July 2023', 'Monday, 24 July 2023', 'Tuesday, 25 July 2023', 'Wednesday, 26 July 2023', 'Thursday, 27 July 2023', 'Friday, 28 July 2023', 'Saturday, 29 July 2023', 'Sunday, 30 July 2023', 'Monday, 31 July 2023', 'Tuesday, 1 August 2023', 'Wednesday, 2 August 2023', 'Thursday, 3 August 2023', 'Friday, 4 August 2023', 'Saturday, 5 August 2023', 'Sunday, 6 August 2023', 'Monday, 7 August 2023', 'Tuesday, 8 August 2023', 'Wednesday, 9 August 2023', 'Thursday, 10 August 2023', 'Friday, 11 August 2023', 'Saturday, 12 August 2023', 'Sunday, 13 August 2023', 'Monday, 14 August 2023', 'Tuesday, 15 August 2023', 'Wednesday, 16 August 2023', 'Thursday, 17 August 2023', 'Friday, 18 August 2023', 'Saturday, 19 August 2023', 'Sunday, 20 August 2023', 'Monday, 21 August 2023', 'Tuesday, 22 August 2023', 'Wednesday, 23 August 2023', 'Thursday, 24 August 2023', 'Friday, 25 August 2023', 'Saturday, 26 August 2023', 'Sunday, 27 August 2023', 'Monday, 28 August 2023', 'Tuesday, 29 August 2023', 'Wednesday, 30 August 2023', 'Thursday, 31 August 2023', 'Friday, 1 September 2023', 'Saturday, 2 September 2023', 'Sunday, 3 September 2023', 'Monday, 4 September 2023', 'Tuesday, 5 September 2023', 'Wednesday, 6 September 2023', 'Thursday, 7 September 2023', 'Friday, 8 September 2023', 'Saturday, 9 September 2023', 'Sunday, 10 September 2023', 'Monday, 11 September 2023', 'Tuesday, 12 September 2023', 'Wednesday, 13 September 2023', 'Thursday, 14 September 2023', 'Friday, 15 September 2023', 'Saturday, 16 September 2023', 'Sunday, 17 September 2023', 'Monday, 18 September 2023', 'Tuesday, 19 September 2023', 'Wednesday, 20 September 2023', 'Thursday, 21 September 2023', 'Friday, 22 September 2023', 'Saturday, 23 September 2023', 'Sunday, 24 September 2023', 'Monday, 25 September 2023', 'Tuesday, 26 September 2023', 'Wednesday, 27 September 2023', 'Thursday, 28 September 2023', 'Friday, 29 September 2023', 'Saturday, 30 September 2023', 'Sunday, 1 October 2023', 'Monday, 2 October 2023', 'Tuesday, 3 October 2023', 'Wednesday, 4 October 2023', 'Thursday, 5 October 2023', 'Friday, 6 October 2023', 'Saturday, 7 October 2023', 'Sunday, 8 October 2023', 'Monday, 9 October 2023', 'Tuesday, 10 October 2023', 'Wednesday, 11 October 2023', 'Thursday, 12 October 2023', 'Friday, 13 October 2023', 'Saturday, 14 October 2023', 'Sunday, 15 October 2023', 'Monday, 16 October 2023', 'Tuesday, 17 October 2023', 'Wednesday, 18 October 2023', 'Thursday, 19 October 2023', 'Friday, 20 October 2023', 'Saturday, 21 October 2023', 'Sunday, 22 October 2023', 'Monday, 23 October 2023', 'Tuesday, 24 October 2023', 'Wednesday, 25 October 2023', 'Thursday, 26 October 2023', 'Friday, 27 October 2023', 'Saturday, 28 October 2023', 'Sunday, 29 October 2023', 'Monday, 30 October 2023', 'Tuesday, 31 October 2023', 'Wednesday, 1 November 2023', 'Thursday, 2 November 2023', 'Friday, 3 November 2023', 'Saturday, 4 November 2023', 'Sunday, 5 November 2023', 'Monday, 6 November 2023', 'Tuesday, 7 November 2023', 'Wednesday, 8 November 2023', 'Thursday, 9 November 2023', 'Friday, 10 November 2023', 'Saturday, 11 November 2023', 'Sunday, 12 November 2023', 'Monday, 13 November 2023', 'Tuesday, 14 November 2023', 'Wednesday, 15 November 2023', 'Thursday, 16 November 2023', 'Friday, 17 November 2023', 'Saturday, 18 November 2023', 'Sunday, 19 November 2023', 'Monday, 20 November 2023', 'Tuesday, 21 November 2023', 'Wednesday, 22 November 2023', 'Thursday, 23 November 2023', 'Friday, 24 November 2023', 'Saturday, 25 November 2023', 'Sunday, 26 November 2023', 'Monday, 27 November 2023', 'Tuesday, 28 November 2023', 'Wednesday, 29 November 2023', 'Thursday, 30 November 2023', 'Friday, 1 December 2023', 'Saturday, 2 December 2023', 'Sunday, 3 December 2023', 'Monday, 4 December 2023', 'Tuesday, 5 December 2023', 'Wednesday, 6 December 2023', 'Thursday, 7 December 2023', 'Friday, 8 December 2023', 'Saturday, 9 December 2023', 'Sunday, 10 December 2023', 'Monday, 11 December 2023', 'Tuesday, 12 December 2023', 'Wednesday, 13 December 2023', 'Thursday, 14 December 2023', 'Friday, 15 December 2023', 'Saturday, 16 December 2023', 'Sunday, 17 December 2023', 'Monday, 18 December 2023', 'Tuesday, 19 December 2023', 'Wednesday, 20 December 2023', 'Thursday, 21 December 2023', 'Friday, 22 December 2023', 'Saturday, 23 December 2023', 'Sunday, 24 December 2023', 'Monday, 25 December 2023', 'Tuesday, 26 December 2023', 'Wednesday, 27 December 2023', 'Thursday, 28 December 2023', 'Friday, 29 December 2023', 'Saturday, 30 December 2023', 'Sunday, 31 December 2023', 'Monday, 1 January 2024', 'Tuesday, 2 January 2024', 'Wednesday, 3 January 2024', 'Thursday, 4 January 2024', 'Friday, 5 January 2024', 'Saturday, 6 January 2024', 'Sunday, 7 January 2024', 'Monday, 8 January 2024', 'Tuesday, 9 January 2024', 'Wednesday, 10 January 2024', 'Thursday, 11 January 2024', 'Friday, 12 January 2024', 'Saturday, 13 January 2024', 'Sunday, 14 January 2024', 'Monday, 15 January 2024', 'Tuesday, 16 January 2024', 'Wednesday, 17 January 2024', 'Thursday, 18 January 2024', 'Friday, 19 January 2024', 'Saturday, 20 January 2024', 'Sunday, 21 January 2024', 'Monday, 22 January 2024', 'Tuesday, 23 January 2024', 'Wednesday, 24 January 2024', 'Thursday, 25 January 2024', 'Friday, 26 January 2024', 'Saturday, 27 January 2024', 'Sunday, 28 January 2024', 'Monday, 29 January 2024', 'Tuesday, 30 January 2024', 'Wednesday, 31 January 2024', 'Thursday, 1 February 2024', 'Friday, 2 February 2024', 'Saturday, 3 February 2024', 'Sunday, 4 February 2024', 'Monday, 5 February 2024', 'Tuesday, 6 February 2024', 'Wednesday, 7 February 2024', 'Thursday, 8 February 2024', 'Friday, 9 February 2024', 'Saturday, 10 February 2024', 'Sunday, 11 February 2024', 'Monday, 12 February 2024', 'Tuesday, 13 February 2024', 'Wednesday, 14 February 2024', 'Thursday, 15 February 2024', 'Friday, 16 February 2024', 'Saturday, 17 February 2024', 'Sunday, 18 February 2024', 'Monday, 19 February 2024', 'Tuesday, 20 February 2024', 'Wednesday, 21 February 2024', 'Thursday, 22 February 2024', 'Friday, 23 February 2024', 'Saturday, 24 February 2024', 'Sunday, 25 February 2024', 'Monday, 26 February 2024', 'Tuesday, 27 February 2024', 'Wednesday, 28 February 2024', 'Thursday, 29 February 2024', 'Friday, 1 March 2024', 'Saturday, 2 March 2024', 'Sunday, 3 March 2024', 'Monday, 4 March 2024', 'Tuesday, 5 March 2024', 'Wednesday, 6 March 2024', 'Thursday, 7 March 2024', 'Friday, 8 March 2024', 'Saturday, 9 March 2024', 'Sunday, 10 March 2024', 'Monday, 11 March 2024', 'Tuesday, 12 March 2024', 'Wednesday, 13 March 2024', 'Thursday, 14 March 2024', 'Friday, 15 March 2024', 'Saturday, 16 March 2024', 'Sunday, 17 March 2024', 'Monday, 18 March 2024', 'Tuesday, 19 March 2024', 'Wednesday, 20 March 2024', 'Thursday, 21 March 2024', 'Friday, 22 March 2024', 'Saturday, 23 March 2024', 'Sunday, 24 March 2024', 'Monday, 25 March 2024', 'Tuesday, 26 March 2024', 'Wednesday, 27 March 2024', 'Thursday, 28 March 2024', 'Friday, 29 March 2024', 'Saturday, 30 March 2024', 'Sunday, 31 March 2024', 'Monday, 1 April 2024', 'Tuesday, 2 April 2024', 'Wednesday, 3 April 2024', 'Thursday, 4 April 2024', 'Friday, 5 April 2024', 'Saturday, 6 April 2024', 'Sunday, 7 April 2024', 'Monday, 8 April 2024', 'Tuesday, 9 April 2024', 'Wednesday, 10 April 2024', 'Thursday, 11 April 2024', 'Friday, 12 April 2024', 'Saturday, 13 April 2024', 'Sunday, 14 April 2024', 'Monday, 15 April 2024', 'Tuesday, 16 April 2024', 'Wednesday, 17 April 2024', 'Thursday, 18 April 2024', 'Friday, 19 April 2024', 'Saturday, 20 April 2024', 'Sunday, 21 April 2024', 'Monday, 22 April 2024', 'Tuesday, 23 April 2024', 'Wednesday, 24 April 2024', 'Thursday, 25 April 2024', 'Friday, 26 April 2024', 'Saturday, 27 April 2024', 'Sunday, 28 April 2024', 'Monday, 29 April 2024', 'Tuesday, 30 April 2024', 'Wednesday, 1 May 2024', 'Thursday, 2 May 2024', 'Friday, 3 May 2024', 'Saturday, 4 May 2024', 'Sunday, 5 May 2024', 'Monday, 6 May 2024', 'Tuesday, 7 May 2024', 'Wednesday, 8 May 2024', 'Thursday, 9 May 2024', 'Friday, 10 May 2024', 'Saturday, 11 May 2024', 'Sunday, 12 May 2024', 'Monday, 13 May 2024', 'Tuesday, 14 May 2024', 'Wednesday, 15 May 2024', 'Thursday, 16 May 2024', 'Friday, 17 May 2024', 'Saturday, 18 May 2024', 'Sunday, 19 May 2024', 'Monday, 20 May 2024', 'Tuesday, 21 May 2024', 'Wednesday, 22 May 2024', 'Thursday, 23 May 2024', 'Friday, 24 May 2024', 'Saturday, 25 May 2024', 'Sunday, 26 May 2024', 'Monday, 27 May 2024', 'Tuesday, 28 May 2024', 'Wednesday, 29 May 2024', 'Thursday, 30 May 2024', 'Friday, 31 May 2024', 'Saturday, 1 June 2024', 'Sunday, 2 June 2024', 'Monday, 3 June 2024', 'Tuesday, 4 June 2024', 'Wednesday, 5 June 2024', 'Thursday, 6 June 2024', 'Friday, 7 June 2024', 'Saturday, 8 June 2024', 'Sunday, 9 June 2024', 'Monday, 10 June 2024', 'Tuesday, 11 June 2024', 'Wednesday, 12 June 2024', 'Thursday, 13 June 2024', 'Friday, 14 June 2024', 'Saturday, 15 June 2024', 'Sunday, 16 June 2024', 'Monday, 17 June 2024', 'Tuesday, 18 June 2024', 'Wednesday, 19 June 2024', 'Thursday, 20 June 2024', 'Friday, 21 June 2024', 'Saturday, 22 June 2024', 'Sunday, 23 June 2024', 'Monday, 24 June 2024', 'Tuesday, 25 June 2024', 'Wednesday, 26 June 2024', 'Thursday, 27 June 2024', 'Friday, 28 June 2024', 'Saturday, 29 June 2024', 'Sunday, 30 June 2024', 'Monday, 1 July 2024', 'Tuesday, 2 July 2024', 'Wednesday, 3 July 2024', 'Thursday, 4 July 2024', 'Friday, 5 July 2024', 'Saturday, 6 July 2024', 'Sunday, 7 July 2024', 'Monday, 8 July 2024', 'Tuesday, 9 July 2024', 'Wednesday, 10 July 2024', 'Thursday, 11 July 2024', 'Friday, 12 July 2024', 'Saturday, 13 July 2024', 'Sunday, 14 July 2024', 'Monday, 15 July 2024', 'Tuesday, 16 July 2024', 'Wednesday, 17 July 2024', 'Thursday, 18 July 2024', 'Friday, 19 July 2024', 'Saturday, 20 July 2024', 'Sunday, 21 July 2024', 'Monday, 22 July 2024', 'Tuesday, 23 July 2024', 'Wednesday, 24 July 2024', 'Thursday, 25 July 2024', 'Friday, 26 July 2024', 'Saturday, 27 July 2024', 'Sunday, 28 July 2024', 'Monday, 29 July 2024', 'Tuesday, 30 July 2024', 'Wednesday, 31 July 2024', 'Thursday, 1 August 2024', 'Friday, 2 August 2024', 'Saturday, 3 August 2024', 'Sunday, 4 August 2024', 'Monday, 5 August 2024', 'Tuesday, 6 August 2024', 'Wednesday, 7 August 2024', 'Thursday, 8 August 2024', 'Friday, 9 August 2024', 'Saturday, 10 August 2024', 'Sunday, 11 August 2024', 'Monday, 12 August 2024', 'Tuesday, 13 August 2024']\n",
      "column_name: 'billed_on', 'datetime64[ns]'\n",
      "synonyms: ['billed on', 'billed_ons', 'invoiced', 'charged', 'billedfor', 'paidfor', 'receivedfee', 'settled', 'remunerated', 'compensated', 'debtedto', 'owedmoney'], datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "# Decalaring Index Schema\n",
    " # Define the schema for the index \n",
    "schema = Schema(column_name=ID(stored=True), synonyms=TEXT(stored=True), unique_values=TEXT(stored=True))  \n",
    "\n",
    "# Create an index directory if it doesn't exist  \n",
    "index_path = f\"{file_name}_kb_index\"\n",
    "if not os.path.exists(index_path):  \n",
    "    os.mkdir(index_path)  \n",
    "  \n",
    "# Create the index  \n",
    "ix = create_in(index_path, schema)  \n",
    "  \n",
    "writer = ix.writer()  \n",
    "  \n",
    "for k,v in schema_dict['data_type'].items():\n",
    "\n",
    "      column_name = f\"'{k}', '{v}'\" \n",
    "      print('column_name:', column_name)\n",
    "\n",
    "      # If it's Edm.DateTime store in 'synonyms' field\n",
    "      if 'datetime' in str(v):\n",
    "        synonyms = f\"{str(schema_dict['synonyms'][k])}, {schema_dict['data_type'][k]}\"\n",
    "        print('synonyms:', synonyms)\n",
    "      else:\n",
    "        synonyms = str(schema_dict['synonyms'][k])\n",
    "        print('synonyms:', synonyms)\n",
    "      # Catch exception in unique_values as only categorical columns are present in it\n",
    "      try:\n",
    "        unique_values = str(schema_dict['categorical_columns'][k])\n",
    "        print(\"unique_values:\", unique_values)\n",
    "      except:\n",
    "        unique_values = ''\n",
    "      \n",
    "      writer.add_document(column_name=column_name, synonyms=synonyms, unique_values=unique_values)\n",
    "  \n",
    "writer.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consuming Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Index\n",
    "file_name = 'jobs_in_data.csv'\n",
    "from whoosh.index import open_dir\n",
    "ix = open_dir(f\"{file_name}_kb_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def identify_column_from_value(ix, keyword, use_wildcard=True):\n",
    "  \n",
    "    \"\"\"\n",
    "    Identify column name from a given value using fuzzy search. \n",
    "    \n",
    "    Parameters:\n",
    "        ix (Index): The index to search in.\n",
    "        keyword (str): The keyword to look for in the unique values of the columns.\n",
    "        use_wildcard (bool): Whether to use wildcards or not. Default is True.\n",
    "            \n",
    "    Returns:\n",
    "        dict: A dictionary where keys are column names and values are a list of unique values that match the keyword.\n",
    "    \"\"\"\n",
    "    identified_column_dict = dict()\n",
    "    with ix.searcher() as searcher:  \n",
    "        ###Fuzzy lookup\n",
    "        parser = QueryParser(\"unique_values\", ix.schema)\n",
    "        parser.add_plugin(FuzzyTermPlugin())\n",
    "        if use_wildcard:\n",
    "            query = parser.parse(f\"*{keyword}*\")  # Add wildcard characters to match any word that contains the keyword\n",
    "        else:\n",
    "            query = parser.parse(keyword)  \n",
    "        ####\n",
    "        results = searcher.search(query, limit=10)\n",
    "        results.fragmenter.charlimit = None\n",
    "        # Show more context before and after\n",
    "        results.fragmenter.surround = 100\n",
    "\n",
    "        for result in results:\n",
    "            # print(result[\"column_name\"], {result.score})\n",
    " \n",
    "            # Remove '</b>' tags  \n",
    "            search_term = result.highlights(\"unique_values\", top=1).replace('</b>', '')  \n",
    "            # Remove '<b class=\"match term[number]\"' tags using regex  \n",
    "            search_term = re.sub(r'<b class=\"match term\\d+\"', '', search_term)  \n",
    "            # Split the text  \n",
    "            search_term = search_term.split(\", '\") \n",
    "            # Remove char '>'\n",
    "            search_term = [re.sub(r'>','',i) for i in search_term if '>' in i]\n",
    "            # print(search_term)\n",
    "            identified_column_dict[result[\"column_name\"]] = search_term\n",
    "\n",
    "    return identified_column_dict\n",
    "\n",
    "identify_column_from_value(ix, \"'Data Scientist'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"'job_category', 'object'\": [\"job category'\", \"job categories'\"],\n",
       " \"'employment_type', 'object'\": ['category']}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from whoosh.qparser import QueryParser, FuzzyTermPlugin\n",
    "\n",
    "def identify_column(ix, keyword):\n",
    "    \"\"\"\n",
    "    Detect column names and the synonyms that matched the keyword.\n",
    "    \n",
    "    Parameters:\n",
    "        ix (Index): The index to search in.\n",
    "        keyword (str): The keyword to look for in the synonyms of the columns.\n",
    "            \n",
    "    Returns:\n",
    "        dict: A dictionary where keys are column names and values are lists of matched synonyms.\n",
    "    \"\"\"\n",
    "    identified_column_dict = dict()\n",
    "\n",
    "    with ix.searcher() as searcher:\n",
    "        # Setup fuzzy lookup with wildcard\n",
    "        parser = QueryParser(\"synonyms\", ix.schema)\n",
    "        parser.add_plugin(FuzzyTermPlugin())\n",
    "        query = parser.parse(f\"*{keyword}*\")\n",
    "        \n",
    "        results = searcher.search(query, limit=30)\n",
    "        results.fragmenter.charlimit = None\n",
    "        results.fragmenter.surround = 100\n",
    "\n",
    "        for result in results:\n",
    "            column_name = result[\"column_name\"]\n",
    "            # Process search highlights to extract matched synonyms\n",
    "            search_term_highlights = result.highlights(\"synonyms\", top=1).replace('</b>', '')\n",
    "            search_term_highlights = re.sub(r'<b class=\"match term\\d+\"', '', search_term_highlights)\n",
    "            search_terms = search_term_highlights.split(\", '\")\n",
    "            search_terms = [re.sub(r'>','',i) for i in search_terms if '>' in i]\n",
    "            \n",
    "            identified_column_dict[column_name] = search_terms\n",
    "\n",
    "    return identified_column_dict\n",
    "\n",
    "identify_column(ix, \"cate\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['show', 'position', 'positions'], False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the file '.txt' and store it in a list with all words in lower case\n",
    "with open(f'{DATA_PATH}/extra_words.txt') as f:\n",
    "    money_words = f.read().splitlines()\n",
    "money_words = [word.lower() for word in money_words]\n",
    "\n",
    "def extract_keywords(text, money_words=[]):  \n",
    "    '''Extract Keywords from the user query'''  \n",
    "    is_date_present = False\n",
    "    # Extract words inside single or double quotes  \n",
    "    quoted_words = re.findall(r'\"([^\"]*)\"|\\'([^\\']*)\\'', text)  \n",
    "    # Flatten the list of tuples and remove empty strings  \n",
    "    quoted_words = [word for words in quoted_words for word in words if word]  \n",
    "  \n",
    "    # Remove quoted words from the original text  \n",
    "    modified_text = re.sub(r'\"[^\"]*\"|\\'[^\\']*\\'', '', text)  \n",
    "  \n",
    "    # Proceed with the existing keyword extraction process  \n",
    "    modified_text = modified_text.lower()  \n",
    "    # Remove date related words\n",
    "    pattern = r\"\\b(?:january|jan|february|feb|march|mar|april|apr|may|june|jun|july|jul|august|aug|september|sep|october|oct|november|nov|december|dec|20[0-4][0-9]|2050|quarter|year|month|ytd|mtd|today|yesterday|tomorrow|qtd)\\b\"  \n",
    "    if re.search(pattern, modified_text, flags=re.IGNORECASE):  \n",
    "        is_date_present = True\n",
    "\n",
    "    modified_text = re.sub(pattern, \"\", modified_text, flags=re.IGNORECASE)  \n",
    "    modified_text = re.sub(r'[^a-zA-Z0-9]', ' ', modified_text)  \n",
    "    modified_text = modified_text.split()  \n",
    "    modified_text = [word for word in modified_text if word not in money_words]  \n",
    "    if is_date_present:\n",
    "        modified_text.extend(['datetime64[', 'datetime32['])\n",
    "    # Combine the quoted words and the extracted keywords  \n",
    "    result = quoted_words + modified_text  \n",
    "  \n",
    "    return result, is_date_present  \n",
    "\n",
    "question = \"show all position positions?\"\n",
    "keywords, is_date_present = extract_keywords(question,money_words)\n",
    "keywords, is_date_present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"'job_title', 'object'\": [\"position'\"], \"'job_category', 'object'\": [\"position'\"], \"'employment_type', 'object'\": [\"position\\\\\\\\_type'\"]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(set(), {})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_columns(ix, keywords):\n",
    "    \"\"\"\n",
    "    Extract columns based on given keywords by searching in attribute synonym knowledge base (kb).\n",
    "    \n",
    "    Parameters:\n",
    "        ix (Index): The index to search in.\n",
    "        keywords (list): A list of keywords to look for in the column names and synonyms.\n",
    "            \n",
    "    Returns:\n",
    "        set, dict: A set of column names detected from the synonym kb and a dictionary mapping each column name to a list of keywords that were matched in it.\n",
    "    \"\"\"\n",
    "    columns_detected = set()\n",
    "    column_keywords_mapping = {}  # Dictionary to store which columns had which keywords detected\n",
    "\n",
    "    for keyword in keywords:\n",
    "        matched_columns = identify_column(ix, keyword)  # Returns a dictionary of column names to matched synonyms\n",
    "        for item in matched_columns.items():\n",
    "            columns_detected.add(item[0])  # Add the column to the set of detected columns\n",
    "            if item[0] not in column_keywords_mapping:\n",
    "                column_keywords_mapping[item[0]] = item[1]\n",
    "            else:\n",
    "                # Add the keyword if not already in the list for this column\n",
    "                if keyword not in column_keywords_mapping[item[0]]:\n",
    "                    column_keywords_mapping[item[0]].append(item[1])\n",
    "\n",
    "    return columns_detected, column_keywords_mapping\n",
    "\n",
    "columns_detected, keywords_matched_columns = extract_columns(ix,keywords)\n",
    "print(keywords_matched_columns)\n",
    "\n",
    "def extract_columns_from_values(ix, keywords):\n",
    "    \"\"\"\n",
    "    Step 2: Looking in attribute synonym kb if keyword not found in synonym kb\n",
    "    Extract columns based on given keywords by searching in attribute unique values knowledge base if keyword not found in synonym kb.\n",
    "    \n",
    "    Parameters:\n",
    "        ix (Index): The index to search in.\n",
    "        keywords (list): A list of keywords to look for in the unique values of the columns.\n",
    "            \n",
    "    Returns:\n",
    "        set, dict: A set of column names detected from the unique values kb and a dictionary mapping each column name to a list of unique values that match any keyword.\n",
    "    \"\"\"\n",
    "    keyword_column_mapping = dict()\n",
    "    columns_detected_from_values = []\n",
    "    for keyword in keywords:\n",
    "        if True:  # keyword not in keywords_matched_columns:\n",
    "            # keyword_column_mapping[keyword] = identify_column_from_value(keyword) # Which word occurs where and how\n",
    "            temp_dict = identify_column_from_value(ix, keyword)\n",
    "            for item in temp_dict.items():\n",
    "                if item[0] not in keyword_column_mapping:\n",
    "                    keyword_column_mapping[item[0]] = item[1]\n",
    "                else:\n",
    "                    keyword_column_mapping[item[0]].extend(item[1])\n",
    "    columns_detected_from_values = list(keyword_column_mapping.keys())\n",
    "    return set(columns_detected_from_values), keyword_column_mapping\n",
    "\n",
    "columns_detected_from_values, keyword_column_mapping = extract_columns_from_values(ix,keywords)\n",
    "columns_detected_from_values, keyword_column_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"'employment_type', 'object'\",\n",
       " \"'job_category', 'object'\",\n",
       " \"'job_title', 'object'\"}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_column_intersection_union(columns_detected,columns_detected_from_values, type='union'):\n",
    "    '''Get Intersection or Union of Columns from Step 1 and Step 2'''\n",
    "    if type == 'union':\n",
    "        return columns_detected_from_values.union(columns_detected)\n",
    "    else:\n",
    "        return columns_detected_from_values.intersection(columns_detected)\n",
    "\n",
    "column_union = get_column_intersection_union(columns_detected,columns_detected_from_values)\n",
    "column_union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'job_title', 'object' -- also referred to as: \"position'\"\n",
      "'job_category', 'object' -- also referred to as: \"position'\"\n",
      "'employment_type', 'object' -- also referred to as: \"position\\\\\\\\_type'\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_table_schema(keyword_column_mapping, keywords_matched_columns, column_union):\n",
    "    schema_string = ''\n",
    "    for column in column_union:\n",
    "        column_data_type = ''#columns_df[columns_df['COLUMN_NAME']==column]['DATA_TYPE'].values[0].upper()\n",
    "        if column in keyword_column_mapping:\n",
    "            schema_string += f\"{column} -- has these unique values: {str(keyword_column_mapping[column])[1:-1]} \\n\"\n",
    "        else:\n",
    "            schema_string += f\"{column} -- also referred to as: {str(keywords_matched_columns[column])[1:-1]}\\n\"\n",
    "    return schema_string\n",
    "\n",
    "schema_string = generate_table_schema(keyword_column_mapping, keywords_matched_columns, column_union)\n",
    "print(schema_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column_name: 'Commodity', 'object'\n",
      "synonyms: ['Commodity', 'Commodities', 'Goods', 'Product', 'Merchandise', 'Stock', 'Asset', 'Tradeable', 'Item', 'Ware', 'CommodityItem', 'Cargo']\n",
      "unique_values: ['Apples', 'Apples, applesauce', 'Apples, ready-to-drink', 'Apples, frozen concentrate', 'Apricots', 'Apricots, packed in juice', 'Apricots, packed in syrup or water', 'Bananas', 'Berries, mixed', 'Blackberries', 'Blueberries', 'Cantaloupe', 'Cherries', 'Cherries, packed in syrup or water', 'Clementines', 'Cranberries', 'Dates', 'Figs', 'Fruit cocktail, packed in juice', 'Fruit cocktail, packed in syrup or water', 'Grapefruit', 'Grapefruit, ready-to-drink', 'Grapes', 'Grapes (raisins)', 'Grapes, ready-to-drink', 'Grapes, frozen concentrate', 'Honeydew', 'Kiwi', 'Mangoes', 'Nectarines', 'Oranges', 'Oranges, ready-to-drink', 'Oranges, frozen concentrate', 'Papaya', 'Peaches', 'Peaches, packed in juice', 'Peaches, packed in syrup or water', 'Pears', 'Pears, packed in juice', 'Pears, packed in syrup or water', 'Pineapple', 'Pineapple, packed in juice', 'Pineapple, packed in syrup or water', 'Pineapple, ready-to-drink', 'Pineapple, frozen concentrate', 'Plum', 'Plum (prunes)', 'Plum (prune), ready-to-drink', 'Pomegranate', 'Pomegranate, ready-to-drink', 'Raspberries', 'Strawberries', 'Watermelon']\n",
      "column_name: 'Form', 'object'\n",
      "synonyms: ['Form', 'Forms', 'Shape', 'Mold', 'Design', 'Layout', 'Structure', 'Scheme', 'Configuration', 'Pattern', 'Blueprint', 'Template']\n",
      "unique_values: ['Fresh', 'Canned', 'Juice', 'Dried', 'Frozen']\n",
      "column_name: 'RetailPrice', 'float64'\n",
      "synonyms: ['Retail Price', 'Retail Prices', 'SalePrice', 'MarketPrice', 'CostPrice', 'TagPrice', 'ListPrice', 'ValuePrice', 'ExchangeRate', 'Fee', 'Tariff', 'Quotation']\n",
      "column_name: 'RetailPriceUnit', 'object'\n",
      "synonyms: ['Retail Price Unit', 'Retail Price Units', 'Price', 'Unit', 'Fee', 'Rate', 'Cost', 'Amount', 'Charge', 'Value', 'Tariff', 'Quota']\n",
      "unique_values: ['per pound', 'per pint']\n",
      "column_name: 'Yield', 'float64'\n",
      "synonyms: ['Yield', 'Yields', 'Production', 'Harvest', 'Return', 'Profit', 'Output', 'Revenue', 'Gain', 'Income', 'Dividend', 'Earnings']\n",
      "column_name: 'CupEquivalentSize', 'float64'\n",
      "synonyms: ['Cup Equivalent Size', 'Cup Equivalent Sizes', 'Volume', 'Capacity', 'Container_size', 'Measure', 'Amount', 'Quantity', 'Size_unit', 'Dimensions', 'Dimension', 'Capacity_unit']\n",
      "column_name: 'CupEquivalentUnit', 'object'\n",
      "synonyms: ['Cup Equivalent Unit', 'Cup Equivalent Units', 'Volume', 'Container', 'Measure', 'Unit_volume', 'Capacity', 'Jug', 'Vessel', 'Pint', 'Quart', 'Milliliter']\n",
      "unique_values: ['pounds', 'fluid ounces']\n",
      "column_name: 'CupEquivalentPrice', 'float64'\n",
      "synonyms: ['Cup Equivalent Price', 'Cup Equivalent Prices', 'Volume_Price', 'Unit_Cost', 'Container_Price', 'Quote', 'Rate', 'Fee', 'Tariff', 'Charge', 'Value', 'Pricing']\n",
      "Index successfully created.\n"
     ]
    }
   ],
   "source": [
    "# # Testing generate_index script\n",
    "from generate_index import generate_index\n",
    "generate_index(file_name='Fruit Prices.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Context\n",
    "import get_dynamic_schema\n",
    "importlib.reload(get_dynamic_schema)\n",
    "print(get_dynamic_schema.get_dynamic_schema(user_query=\"overall sales of envelope in 2022, xerox consumer standard second?\", search_mode='strict',file_name=\"Global Superstore.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataframe\n",
    "import pandas as pd\n",
    "file_name = 'jobs_in_data.csv'\n",
    "df = pd.read_csv(file_name)\n",
    "top_3_rows = str(df.head(3).to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    ### Boss's Question:\n",
      "    'Data Scientist''\n",
      "    ### Some values and their respective columns (if detected) from the Question are:\n",
      "    'job_title', 'object' -- has these unique values: \"Data Analytics Specialist'\", \"Big Data Architect'\", \"Marketing Data Engineer'\", \"Manager Data Management'\", \"Data Analytics Consultant'\", \"Data Science Tech Lead'\", \"Data Scientist Lead'\", \"Marketing Data Analyst'\", 'Data Analytics' \n",
      "\n",
      "    ## Instructions:\n",
      "    It is in the format <column-name>, <data-type>, <unique-values>. Use the columns mentioned, their synonyms and it's unique values to frame your code in order to answer the question as correctly as possible.\n",
      "    The dataframe in question has already been loaded as 'df', so you don't need to do it(i.e. never use the load data code).\n",
      "\n",
      "    Always keep the python code(executable code only, no description) in this format: ```python <generated-code> ```\n",
      "    There's no need to show the output or output might look because Boss will execute the generated code himself. \n",
      "    Always include a print() statement in the code to print the final result along with some text to frame your answer to Boss.\n",
      "\n",
      "    No Code Explanation/Description is required at all. Just the executable python code should be your output. Don't talk back to Boss.\n",
      "\n",
      "    ### Your Code: df = \n"
     ]
    }
   ],
   "source": [
    "import generate_prompt\n",
    "importlib.reload(generate_prompt)\n",
    "\n",
    "prompt, db_query, token_usage = generate_prompt.generate_prompt(\n",
    "    user_query= \"'Data Scientist''\", \n",
    "    file_name=file_name, \n",
    "    topk_rows=top_3_rows, \n",
    "    search_mode='flexi'\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did you mean(identify_column): in-person?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"what is united 'In-person' <'work_setting'>'\"]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import search_suggestions\n",
    "importlib.reload(search_suggestions)\n",
    "def get_suggestions(ix, user_query:str, enable_fuzzy:bool):\n",
    "    \"\"\"\n",
    "    Get a list of sentence suggestions based on the user's query and index. If fuzzy search is enabled, use it to generate suggestions; otherwise, enable spell correction and fuzzy searching.\n",
    "\n",
    "    Args:\n",
    "    - ix (inverted index): The inverted index used for generating sentence suggestions.\n",
    "    - user_query (str): The input user's query.\n",
    "    - enable_fuzzy (bool): A flag to determine whether to use fuzzy search or not.\n",
    "\n",
    "    Returns:\n",
    "    - list of str: A list of suggested sentences based on the user's query and index, with a maximum length of 5.\n",
    "    \"\"\"\n",
    "\n",
    "    if enable_fuzzy:\n",
    "        suggested_sentences = search_suggestions.suggest_sentences(\n",
    "            ix=ix, input_sentence=user_query, top_k=5\n",
    "        )\n",
    "    else:\n",
    "        suggested_sentences = search_suggestions.suggest_sentences(\n",
    "            ix=ix,\n",
    "            input_sentence=user_query,\n",
    "            top_k=5,\n",
    "            enable_spell_correct=True,\n",
    "            enable_fuzzy=enable_fuzzy,\n",
    "        )\n",
    "    return suggested_sentences\n",
    "\n",
    "get_suggestions(ix, \"what is united in-person \", enable_fuzzy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quoted Words: [('', 'Commodity'), ('', 'Apples'), ('', 'Commodity')]\n",
      "Modified Text: show all  without  <>' \n",
      "Extracted Keywords: ['show']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['show']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_keywords(text, money_words=[]):  \n",
    "    '''Extract Keywords from the user query'''  \n",
    "\n",
    "    # Load Keywords\n",
    "    with open(f'extra_words.txt') as f:\n",
    "        money_words = f.read().splitlines()\n",
    "    money_words = [word.lower() for word in money_words]\n",
    "\n",
    "    is_date_present = False\n",
    "    # Extract words inside single or double quotes  \n",
    "    quoted_words = re.findall(r'\"([^\"]*)\"|\\'([^\\']*)\\'', text)  \n",
    "    # quoted_words = re.findall(r'\"([^\"]*)\"|\\'([^\\']*)\\'|\\'\\'([^\\']*)\\'\\'', text)\n",
    "\n",
    "\n",
    "    print(f\"Quoted Words: {quoted_words}\")\n",
    "\n",
    "    # Flatten the list of tuples and remove empty strings  \n",
    "    quoted_words = [word for words in quoted_words for word in words if word]  \n",
    "  \n",
    "    # Remove quoted words from the original text  \n",
    "    modified_text = re.sub(r'\"[^\"]*\"|\\'[^\\']*\\'', '', text)  \n",
    "    # modified_text = re.sub(r'\"[^\"]*\"|\\'[^\\']*\\'|\\'\\'[^\\']*\\'\\'', '', text)\n",
    "\n",
    "  \n",
    "    # Proceed with the existing keyword extraction process  \n",
    "    modified_text = modified_text.lower()  \n",
    "\n",
    "    print(f\"Modified Text: {modified_text}\")\n",
    "\n",
    "    # Remove date related words\n",
    "    pattern = r\"\\b(?:january|jan|february|feb|march|mar|april|apr|may|june|jun|july|jul|august|aug|september|sep|october|oct|november|nov|december|dec|20[0-4][0-9]|2050|quarter|year|month|ytd|mtd|today|yesterday|tomorrow|qtd)\\b\"  \n",
    "    if re.search(pattern, modified_text, flags=re.IGNORECASE):  \n",
    "        is_date_present = True\n",
    "\n",
    "    modified_text = re.sub(pattern, \"\", modified_text, flags=re.IGNORECASE)  \n",
    "    modified_text = re.sub(r'[^a-zA-Z0-9]', ' ', modified_text)  \n",
    "    modified_text = modified_text.split()  \n",
    "    modified_text = [word for word in modified_text if word not in money_words]  \n",
    "    if is_date_present:\n",
    "        modified_text.extend(['datetime64[', 'datetime32['])\n",
    "    # Combine the quoted words and the extracted keywords  \n",
    "    result =  modified_text # + quoted_words \n",
    "\n",
    "    print(f\"Extracted Keywords: {result}\")\n",
    "  \n",
    "    return result, is_date_present  \n",
    "\n",
    "result, is_date_present  = extract_keywords(f\"\"\"show all ''Commodity'' without 'Apples' <'Commodity'>' \"\"\".replace(\"''\",\"'\"))\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "table_talks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
